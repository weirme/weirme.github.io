<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Support Vector Machine 2" /><meta property="og:locale" content="en" /><meta name="description" content="Soft-Margin SVM" /><meta property="og:description" content="Soft-Margin SVM" /><link rel="canonical" href="https://weirme.github.io/posts/Support-Vector-Machine-2/" /><meta property="og:url" content="https://weirme.github.io/posts/Support-Vector-Machine-2/" /><meta property="og:site_name" content="weirme" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2018-08-10T00:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Support Vector Machine 2" /><meta name="twitter:site" content="@weirmezz" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-05-16T19:52:23+08:00","datePublished":"2018-08-10T00:00:00+08:00","description":"Soft-Margin SVM","headline":"Support Vector Machine 2","mainEntityOfPage":{"@type":"WebPage","@id":"https://weirme.github.io/posts/Support-Vector-Machine-2/"},"url":"https://weirme.github.io/posts/Support-Vector-Machine-2/"}</script><title>Support Vector Machine 2 | weirme</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="weirme"><meta name="application-name" content="weirme"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fastly.jsdelivr.net" ><link rel="dns-prefetch" href="https://fastly.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://fastly.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src=" https://raw.githubusercontent.com/weirme/picme/main/DC751EAE-6C3C-4052-B734-5EBD6D3A89B2.jpeg " alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">weirme</a></div><div class="site-subtitle font-italic">rational & independent</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/weirme" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/weirmezz" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['weirme','foxmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Support Vector Machine 2</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Support Vector Machine 2</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/weirme">weirme</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1533830400" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2018-08-10 </em> </span> <span> Updated <em class="timeago" data-ts="1652701943" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-05-16 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1603 words"> <em>8 min</em> read</span></div></div></div><div class="post-content"><h2 id="soft-margin-svm"><span class="mr-2">Soft-Margin SVM</span><a href="#soft-margin-svm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>In order to avoid the effect of noise in dataset, we allow the model to make a small number of mistakes, similar to PLA, we have a new optimization goal shaped like</p>\[\begin{split} \min_{b,\boldsymbol w} \ \ &amp;\frac{1}{2}\boldsymbol w^\text T\boldsymbol w+C \cdot \sum_{n=1}^N[\![y_n \ne \text{sign}(\boldsymbol w^\text T\boldsymbol x+b)]\!] \\[1em] \text{s.t.} \ \ &amp;y_n(\boldsymbol w^\text T\boldsymbol x_n+b)\ge 1-\infty \ \cdot [\![y_n \ne \text{sign}(\boldsymbol w^\text T\boldsymbol x+b)]\!], \ n=1,2,...,N \end{split} \tag{9.31}\]<p>the constraint means we only care about points separated correctly, for these error point we take them into the penalty in our goal and use constant $C$ to set our tolerance for errors. However, there are two cons of our model. Firstly it is non-linear, and secondly the penalty of this model cannot distinguish points with small error or large error. In order to solve the two problems, we modify our goal as follows:</p>\[\begin{split} \min_{b,\boldsymbol w,\boldsymbol\xi} \ \ &amp;\frac{1}{2}\boldsymbol w^\text T\boldsymbol w+C \cdot \sum_{n=1}^N\xi_n \\[1em] \text{s.t.} \ \ &amp;y_n(\boldsymbol w^\text T\boldsymbol x_n+b)\ge 1-\xi_n \\[1em] &amp;\xi_n \ge 0, \ n=1,2,...,N \end{split} \tag{9.32}\]<p>where $\xi_n$ describes the ‘margin violation’ , in other words the degree of error in sample point.</p><p>Next we construct Lagrange dual problem similar as before. With an increased constraints, we set two Lagrange multipliers $\alpha_n$ and $\beta_n$. Then we have Lagrange function</p>\[\mathcal L(b,\boldsymbol w,\boldsymbol\xi,\boldsymbol\alpha,\boldsymbol\beta) = \frac{1}{2}\boldsymbol w^\text T\boldsymbol w+C \cdot \sum_{n=1}^N\xi_n + \sum_{n=1}^N\alpha_n\left(1-\xi_n-y_n(\boldsymbol w^\text T\boldsymbol x_n+b)\right) + \sum_{n=1}^N\beta_n(-\xi_n) \tag{9.33}\]<p>and the dual problem is</p>\[\max_{\alpha_n \ge 0, \beta_n \ge 0} \ \min_{b,\boldsymbol w,\boldsymbol\xi} \ \ \mathcal L(b,\boldsymbol w,\boldsymbol\xi,\boldsymbol\alpha,\boldsymbol\beta) \tag{9.34}\]<p>Firstly, we set $\nabla_{\xi_n}\mathcal L = 0$, we have</p>\[C-\alpha_n-\beta_n=0 \tag{9.35}\]<p>which means we can eliminate $\beta_n$ with $C-\alpha_n$, and add a new constraint $0 \le \alpha_n \le C$.</p>\[\begin{split} \mathcal L(b,\boldsymbol w,\boldsymbol\xi,\boldsymbol\alpha,\boldsymbol\beta) &amp;= \frac{1}{2}\boldsymbol w^\text T\boldsymbol w+C \cdot \sum_{n=1}^N\xi_n - \sum_{n=1}^N\alpha_n\xi_n + \sum_{n=1}^N\alpha_n\left(1-y_n(\boldsymbol w^\text T\boldsymbol x_n+b)\right) + \sum_{n=1}^N(C-\alpha_n)(-\xi_n) \\[1em] &amp;=\frac{1}{2}\boldsymbol w^\text T\boldsymbol w + \sum_{n=1}^N\alpha_n\left(1-y_n(\boldsymbol w^\text T\boldsymbol x_n+b)\right) \end{split} \tag{9.36}\]<p>and we find $\xi_n$ is also eliminated in this transform, the problem has become very close to that in hard-margin SVM. After derivation, we can get a similar form as follows:</p>\[\begin{split} &amp;\min_{\boldsymbol \alpha} \ \frac{1}{2}\sum_{n=1}^N\sum_{m=1}^N\alpha_n\alpha_my_ny_m\boldsymbol x_n^\text T\boldsymbol x_m - \sum_{n=1}^N \alpha_n \\[1em] &amp;\text{s.t.} \begin{cases} 0 \le \alpha_n \le C \\[1em] \sum_{n=1}^N\alpha_ny_n=0 \\[1em] \boldsymbol w=\sum_{n=1}^N\alpha_ny_n\boldsymbol x_n, \ \beta_n=C-\alpha_n \\[1em] \end{cases} \end{split} \tag{9.37}\]<p>As we did in hard-margin SVM, we have constraints $\alpha_n\left(1-\xi_n-y_n(\boldsymbol w^\text T\boldsymbol x_n+b)\right)=0$ and $(C-\alpha_n)\xi_n=0$. We try to find a ‘free’ support vector $\boldsymbol x_s$ such that $0&lt;\alpha_s&lt;C$, we get</p>\[\begin{split} b &amp;= y_s-\boldsymbol w^\text T\boldsymbol x_s \\[1em] &amp;=y_s-\sum_{n=1}^N\alpha_ny_nK(\boldsymbol x_n,\boldsymbol x_s) \end{split} \tag{9.38}\]<p>According to the specific $\alpha_n$, we can judge the position of sample $\boldsymbol x_n$.</p><p><img data-src="https://raw.githubusercontent.com/weirme/picme/main/15.png" width="40%" data-proofer-ignore></p><ul><li>when $\alpha_n=0$, then $\xi_n=0$. The sample is non support vector which should be away from the boundary.<li>when $0&lt;\alpha_n&lt;C$, then $\xi_n=0$. The sample is a free support vector located on the boundary. (square in figure)<li>when $\alpha_n=C$, then $\xi_n&gt;0$ describes violation amount. The sample violates the boundary. (triangle in figure)</ul><p>At the same time, we find that $\xi_n=1-y_n(\boldsymbol w^\text T\boldsymbol x_n+b)$ if $\boldsymbol x_n$ violates the margin and else $\xi_n=0$. In a brief form, we have</p>\[\xi_n=\max\left(1-y_n(\boldsymbol w^\text T\boldsymbol x_n+b),0\right) \tag{9.39}\]<p>according to this formula, we can rewrite our objective in $(9.32)$ as follows:</p>\[\min_{b,\boldsymbol w} \ \ \frac{1}{2}\boldsymbol w^\text T\boldsymbol w+C \cdot \sum_{n=1}^N\max\left(1-y_n(\boldsymbol w^\text T\boldsymbol x_n+b),0\right) \tag{9.40}\]<p>Consider the first item in this formula as L-2 regularizer and the second item describes the degree of error, then the soft-margin SVM can be classified as a kind of a unique form of regularization.</p><h2 id="kernel-logistic-regression"><span class="mr-2">Kernel Logistic Regression</span><a href="#kernel-logistic-regression" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>Kernel tricks help us save expense of calculating in high dimension space, it can also be applied into logistic regression like a ‘two-level learning’.</p><p>With feature transformation $\boldsymbol\Phi(\boldsymbol x)$, the optimal hypothesis of logistic regression is written as</p>\[g(\boldsymbol x) = \sigma(\boldsymbol w^\text T\boldsymbol\Phi(\boldsymbol x)+b) \tag{9.41}\]<p>according to previous derivation, we have</p>\[\boldsymbol w^\text T\boldsymbol\Phi(\boldsymbol x)+b=\sum_{n=1}^N\alpha_ny_nK_{\boldsymbol\Phi}(\boldsymbol x_n,\boldsymbol x)+y_s-\sum_{n=1}^N\alpha_ny_nK_{\boldsymbol\Phi}(\boldsymbol x_n,\boldsymbol x_s) \tag{9.42}\]<p>view $\boldsymbol w^\text T\boldsymbol\Phi(\boldsymbol x)+b$ as a special transform, recorded as $\Phi_\text{SVM}(\boldsymbol x)$, which can easily calculted with kernel tricks, then $g$ becomes</p>\[g(\boldsymbol x)=\sigma\left(A\cdot\Phi_\text{SVM}(\boldsymbol x)+B\right) \tag{9.43}\]<p>where $A$ describes scaling and $B$ describes shifting, then the optimization goal in logistic regression becomes</p>\[\min_{A,B} \ \frac{1}{N}\sum_{n=1}^N\ln\left(1+\exp\left(-y_n(A\cdot\Phi_\text{SVM}(\boldsymbol x)+B\right)\right) \tag{9.44}\]<p>then the problem has reduced to a 1st-dim logistic regression with only two variables.</p><p>This method finding a transform $\Phi_\text{SVM}(\boldsymbol x)$ in a high space and return a scalar value, next we try to do exact logistic regression in the high space with kernel. Consider L-2 regularized logistic regression, the optimization goal is</p>\[\min_{\boldsymbol w} \ \frac{1}{N}\sum_{n=1}^N\ln(1+ \exp (-y_n\boldsymbol w^{\text T}\boldsymbol x_n)) + \frac{\lambda}{N}\boldsymbol w^\text T\boldsymbol w \tag{9.45}\]<p>from which we can ensure the final $\boldsymbol w$ is the linear combination of $\boldsymbol x_n$, and we can further prove it established for each L-2 regularized linear model</p>\[\min_{\boldsymbol w} \ \frac{\lambda}{N}\boldsymbol w^\text T\boldsymbol w+\frac{1}{N}\sum_{n=1}^N\text{err}(y_n, \boldsymbol w^\text T\boldsymbol x) \tag{9.46}\]<p>the optimal $\boldsymbol w$ can be written as $\boldsymbol w_\ast=\sum_{n=1}^N\beta_n\boldsymbol x_n$. Assume we have a optimal $\boldsymbol w_\ast$ violates this equality, and $\boldsymbol w_\ast=\boldsymbol w_\bot+\boldsymbol w_\Vert$, where $\boldsymbol w_\bot \in \text{ span}(\boldsymbol x_n)$ and $\boldsymbol w_\Vert \bot \text{ span}(\boldsymbol x_n)$. then we have</p>\[\text{err}(y_n, \boldsymbol w_*^\text T\boldsymbol x)=\text{err}(y_n, (\boldsymbol w_\bot+\boldsymbol w_\Vert)^\text T\boldsymbol x) = \text{err}(y_n, \boldsymbol w_\Vert^\text T\boldsymbol x) \tag{9.47}\]<p>another hand</p>\[\begin{split} \boldsymbol w_*^\text T\boldsymbol w_* &amp;= \boldsymbol w_\bot^\text T\boldsymbol w_\bot + 2\boldsymbol w_\bot^\text T\boldsymbol w_\Vert + \boldsymbol w_\Vert^\text T\boldsymbol w_\Vert \\[1em] &amp;= \boldsymbol w_\bot^\text T\boldsymbol w_\bot + \boldsymbol w_\Vert^\text T\boldsymbol w_\Vert \\[1em] &amp;\ge \boldsymbol w_\Vert^\text T\boldsymbol w_\Vert \end{split} \tag{9.48}\]<p>so we have a better $\boldsymbol w_\Vert$ more optimal than $\boldsymbol w_*$ which is a contradiction.</p><p>With this condition, substitute the optimal $\boldsymbol w_*=\sum_{i=1}^N\beta_i\boldsymbol x_i$ into the goal function, we have</p>\[\begin{split} \boldsymbol w^\text T\boldsymbol x_n &amp;= \sum_{i=1}^N\beta_i\boldsymbol x_i^\text T\boldsymbol x_n = \sum_{n=1}^N\beta_iK(\boldsymbol x_i,\boldsymbol x_n) \\[1em] \boldsymbol w^\text T\boldsymbol w &amp;= \sum_{i=1}^N\beta_i\boldsymbol x_i^\text T \cdot \sum_{i=1}^N\beta_i\boldsymbol x_i \\[1em] &amp;= \sum_{i=1}^N\sum_{j=1}^N\beta_i\beta_jK(\boldsymbol x_i,\boldsymbol x_j) \end{split} \tag{9.49}\]<p>then the problem becomes finding a optimal $\boldsymbol\beta$ instead of $\boldsymbol w$</p>\[\min_{\boldsymbol \beta} \ \frac{1}{N}\sum_{n=1}^N\ln\left(1+ \exp \left(-y_n\sum_{n=1}^N\beta_iK(\boldsymbol x_i,\boldsymbol x_n)\right)\right) + \frac{\lambda}{N}\sum_{i=1}^N\sum_{j=1}^N\beta_i\beta_jK(\boldsymbol x_i,\boldsymbol x_j)\tag{9.50}\]<p>this form is called <strong>Kernel Logistic Regression</strong>, easily find it is quite similar with SVM.</p><h2 id="support-vector-regression"><span class="mr-2">Support Vector Regression</span><a href="#support-vector-regression" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>In general rigde regression, we measure the error in training dataset with square error $(y_n-\boldsymbol w^\text T\boldsymbol x_n)^2$, according to our derivation above, it can be rewritten with kernel as $\left(y_n-\sum_{i=1}^N\beta_iK(\boldsymbol x_i,\boldsymbol x_n)\right)^2$. Then our optimization goal is</p>\[\min_{\boldsymbol \beta} \ \frac{1}{N}\sum_{n=1}^N\left(y_n-\sum_{i=1}^N\beta_iK(\boldsymbol x_i,\boldsymbol x_n)\right)^2 + \frac{\lambda}{N}\sum_{i=1}^N\sum_{j=1}^N\beta_i\beta_jK(\boldsymbol x_i,\boldsymbol x_j)\tag{9.51}\]<p>this complicated formula can be simplified with matrix form as follows:</p>\[\begin{split} &amp;\min_{\boldsymbol\beta} \ \frac{1}{N}\Vert\boldsymbol y-\boldsymbol {K\beta}\Vert + \frac{\lambda}{N}\boldsymbol\beta^\text T\boldsymbol{K\beta} \\[1em] =&amp; \min_{\boldsymbol\beta} \frac{1}{N}\left(\boldsymbol\beta^\text T\boldsymbol K^\text T\boldsymbol{K\beta}-2\boldsymbol\beta^\text T\boldsymbol K^\text T\boldsymbol y + \boldsymbol y^\text T\boldsymbol y \right) + \frac{\lambda}{N}\boldsymbol\beta^\text T\boldsymbol{K\beta} \end{split} \tag{9.52}\]<p>record this objective as $E_\text{aug}$ and then differentiate $\boldsymbol\beta$, we have</p>\[\begin{split} \nabla_{\boldsymbol\beta}E_\text{aug} &amp;= \frac{2}{N}\left(\lambda\boldsymbol {K\beta}+\boldsymbol K^\text T\boldsymbol {K\beta}-\boldsymbol K^\text T\boldsymbol y\right) \\[1em] &amp;= \frac{2}{N}\boldsymbol K^\text T\left((\lambda\boldsymbol I+\boldsymbol K)\boldsymbol\beta-\boldsymbol y\right) \end{split} \tag{9.53}\]<p>set $\nabla_{\boldsymbol\beta}E_\text{aug}=0$, we have one analytic solution</p>\[\boldsymbol\beta=(\lambda\boldsymbol I+\boldsymbol K)^{-1}\boldsymbol y \tag{9.54}\]<p>this inverse matrix always exists because we have $\lambda&gt;0$ and $\boldsymbol K$ is positive semi-definite. Complexity of train with kernel regression is $O(N^3)$, which is quite high for big data, and in that case, linear regression may be a good alternatives.</p><p>Similar to soft-margin SVM, we consider a special ‘tube’ regression</p><p><img data-src="https://raw.githubusercontent.com/weirme/picme/main/16.png" width="40%" data-proofer-ignore></p><p>as shown in above image, points in the tube is regarded as no error while points outside the tube have a error measured by distance to tube. That is</p>\[\text{err}(y,\hat y)= \begin{cases} 0 &amp; |\hat y-y| \le \epsilon \\[1em] |\hat y-y|-\epsilon &amp; |\hat y-y| &gt; \epsilon \end{cases} \tag{9.55}\]<p>where $\hat y=\boldsymbol w^\text T\boldsymbol x_n+b$, in general we have</p>\[\text{err}(y, \hat y)= \max(0,|\hat y-y|-\epsilon) \tag{9.56}\]<p>this form of error is called <strong>$\epsilon$-insentive Error</strong> with $\epsilon&gt;0$. Then our optimization goal is</p>\[\min_{b,\boldsymbol w} \ C\sum_{n=1}^N\max(0, \vert\hat y_n-y_n\vert - \epsilon)+\frac{1}{2}\boldsymbol w^\text T\boldsymbol w \tag{9.57}\]<p>the $\max$ function is non-differentiable in some point, we introduce a slack variable $\xi_n = \max(0, \vert \hat{y}_n - y_n \vert - \epsilon)$ similar to soft-margin SVM, and rewrite $(9.57)$</p>\[\begin{split} \min_{b,\boldsymbol w,\boldsymbol\xi} \ &amp;C\sum_{n=1}^N\xi_n+\frac{1}{2}\boldsymbol w^\text T\boldsymbol w \\[1em] \text{s.t.} \ &amp;|\hat y_n-y_n| \le \epsilon+\xi_n \\[1em] &amp; \xi_n \ge 0 \end{split} \tag{9.58}\]<p>in this formula, when $\vert\hat y_n-y_n\vert-\epsilon \le 0$, in order to minimize the objective, it should have $\xi_n=0$, and when $\vert\hat y_n-y_n\vert - \epsilon &gt; 0$, then $\xi_n \ge \vert\hat y_n-y_n\vert-\epsilon$ according to the first constraint, to minimize the objective, we have $\xi_n = \vert\hat y_n-y_n\vert-\epsilon$. This shows that introducing this variable is reasonable. Further, remove absolute value and transform the constraint into linear form</p>\[\begin{split} \min_{b,\boldsymbol w,\boldsymbol\xi^&lt;,\boldsymbol\xi^&gt;} \ &amp;C\sum_{n=1}^N(\xi_n^&lt;+\xi_n^&gt;)+\frac{1}{2}\boldsymbol w^\text T\boldsymbol w \\[1em] \text{s.t.} \ &amp;\boldsymbol w^\text T\boldsymbol x_n+b-y_n \le \epsilon+\xi_n^&gt; \\[1em] &amp;\boldsymbol w^\text T\boldsymbol x_n+b-y_n \ge -\epsilon-\xi_n^&lt; \\[1em] &amp; \xi_n^&lt; \ge 0, \ \xi_n^&gt; \ge 0 \end{split} \tag{9.59}\]<p>where $\xi_n^&lt;$ is the lower tube violations while $\xi_n^&gt;$ is the upper tube violations. Introduce Lagrange multipliers $\alpha^&lt;_n$ and $\alpha^&gt;_n$, then do some calculations as before, we can have some of the KKT conditions:</p>\[\begin{align} \boldsymbol w &amp;= \sum_{n=1}^N(\alpha_n^&lt;-\alpha_n^&gt;)\boldsymbol x_n \tag{9.60} \\[1em] 0 &amp;= \sum_{n=1}^N(\alpha_n^&lt;-\alpha_n^&gt;) \tag{9.61} \\[1em] 0 &amp;= \alpha_n^&lt;(\boldsymbol w^\text T\boldsymbol x_n+b-y_n+\epsilon+\xi_n^&lt;) \tag{9.62} \\[1em] 0 &amp;= \alpha_n^&gt;(\boldsymbol w^\text T\boldsymbol x_n+b-y_n-\epsilon-\xi_n^&gt;) \tag{9.63} \\[1em] \end{align}\]<p>similarly, we can get the dual probblem of SVR, which can be solved with QP computing program. According to formula $(9.62)$ and $(9.63)$, we have the following two situations:</p><ul><li><p>For points within tube, we have</p>\[\begin{split} &amp;|\boldsymbol w^\text T\boldsymbol x_n+b-y_n| \le \epsilon \\[1em] \Rightarrow \ &amp; \xi_n^&lt;=0, \ \xi_n^&gt;=0 \\[1em] \Rightarrow \ &amp; \boldsymbol w^\text T\boldsymbol x_n+b-y_n+\epsilon+\xi_n^&lt; \ne 0, \ \boldsymbol w^\text T\boldsymbol x_n+b-y_n-\epsilon-\xi_n^&gt; \ne 0 \\[1em] \Rightarrow \ &amp;\alpha_n^&lt;=0, \ \alpha_n^&gt;=0 \end{split} \tag{9.64}\]<p>clearly, these points make no contribution to the final $\boldsymbol w$.</p><li><p>For points outside tube, which are support vectors in this model, we can calculate $\boldsymbol w$ and $b$ through these points.</p></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/notes/'>Notes</a>, <a href='/categories/machine-learning/'>Machine Learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/ml/" class="post-tag no-text-decoration" >ML</a> <a href="/tags/svm/" class="post-tag no-text-decoration" >SVM</a> <a href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" class="post-tag no-text-decoration" >支持向量机</a> <a href="/tags/%E5%88%86%E7%B1%BB/" class="post-tag no-text-decoration" >分类</a> <a href="/tags/%E5%9B%9E%E5%BD%92/" class="post-tag no-text-decoration" >回归</a> <a href="/tags/%E7%AE%97%E6%B3%95/" class="post-tag no-text-decoration" >算法</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Support Vector Machine 2 - weirme&amp;url=https://weirme.github.io/posts/Support-Vector-Machine-2/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Support Vector Machine 2 - weirme&amp;u=https://weirme.github.io/posts/Support-Vector-Machine-2/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://weirme.github.io/posts/Support-Vector-Machine-2/&amp;text=Support Vector Machine 2 - weirme" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/%E6%9E%84%E9%80%A0-%E6%9E%90%E6%9E%84-%E8%B5%8B%E5%80%BC/">构造-析构-赋值</a><li><a href="/posts/%E4%BB%8EC%E5%88%B0C++/">从 C 到 C++</a><li><a href="/posts/Support-Vector-Machine-1/">Support Vector Machine 1</a><li><a href="/posts/Support-Vector-Machine-2/">Support Vector Machine 2</a><li><a href="/posts/Radial-Basic-Function-Network/">Radial Basic Function Network</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ml/">ML</a> <a class="post-tag" href="/tags/%E7%AE%97%E6%B3%95/">算法</a> <a class="post-tag" href="/tags/%E5%88%86%E7%B1%BB/">分类</a> <a class="post-tag" href="/tags/effective-c/">Effective C++</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/svm/">SVM</a> <a class="post-tag" href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">支持向量机</a> <a class="post-tag" href="/tags/boosting/">Boosting</a> <a class="post-tag" href="/tags/k-means/">K-Means</a> <a class="post-tag" href="/tags/matplotlib/">Matplotlib</a></div></div></div><script src="https://fastly.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Support-Vector-Machine-1/"><div class="card-body"> <em class="timeago small" data-ts="1533484800" > 2018-08-06 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Support Vector Machine 1</h3><div class="text-muted small"><p> Linear SVM Large-Margin Problem Firstly, we consider 3 linear classifier on the same dataset as follows: both of them seem performing well but the rightmost one whose hyperplane is farthest fr...</p></div></div></a></div><div class="card"> <a href="/posts/Linear-Model-for-Classification/"><div class="card-body"> <em class="timeago small" data-ts="1531411200" > 2018-07-13 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Linear Model for Classification</h3><div class="text-muted small"><p> Summary of Error Function All three algorithm we mentioned above have the same linear scoring function $s=\boldsymbol w^{\text T}\boldsymbol x$, now we discuss about the similarity of these algori...</p></div></div></a></div><div class="card"> <a href="/posts/Adaptive-Boosting/"><div class="card-body"> <em class="timeago small" data-ts="1534089600" > 2018-08-13 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Adaptive Boosting</h3><div class="text-muted small"><p> AdaBoost is an algorithm that promotes weak learners to a strong learner. This algorithm first trains a base learner from the initial training set, and then adjusts the sample distribution accordin...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Python%E5%9F%BA%E7%A1%80/" class="btn btn-outline-primary" prompt="Older"><p>Python基础</p></a> <a href="/posts/Adaptive-Boosting/" class="btn btn-outline-primary" prompt="Newer"><p>Adaptive Boosting</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/weirme">weirme</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ml/">ML</a> <a class="post-tag" href="/tags/%E7%AE%97%E6%B3%95/">算法</a> <a class="post-tag" href="/tags/%E5%88%86%E7%B1%BB/">分类</a> <a class="post-tag" href="/tags/effective-c/">Effective C++</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/svm/">SVM</a> <a class="post-tag" href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">支持向量机</a> <a class="post-tag" href="/tags/boosting/">Boosting</a> <a class="post-tag" href="/tags/k-means/">K-Means</a> <a class="post-tag" href="/tags/matplotlib/">Matplotlib</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://fastly.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://fastly.jsdelivr.net/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script src="https://fastly.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script src="https://fastly.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://fastly.jsdelivr.net/npm/dayjs@1/dayjs.min.js"></script> <script src="https://fastly.jsdelivr.net/npm/dayjs@1/locale/en.min.js"></script> <script src="https://fastly.jsdelivr.net/npm/dayjs@1/plugin/relativeTime.min.js"></script> <script src="https://fastly.jsdelivr.net/npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://fastly.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
