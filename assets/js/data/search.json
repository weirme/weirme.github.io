[ { "title": "资源管理", "url": "/posts/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/", "categories": "Notes, C/C++", "tags": "Effective C++", "date": "2022-05-18 10:17:00 +0800", "snippet": "以对象管理资源当我们向系统申请某一笔资源后, 应该立刻在同一语句内用它来初始化某个管理对象. 如果有下面这个工厂函数用来动态分配一个资源, 并返回指向资源的指针:Resource* create_resource();大部分时候我们可以通过智能指针很好的管理资源, 例如下面的代码:std::shared_ptr&amp;lt;Resource&amp;gt; ptr(create_resource());这样做的好处是将返回的指针放入智能指针对象中进行管理, 这样当该智能指针对象的生存期结束后, 将会在其析构函数中自动释放其持有的资源. 如果不这么做, 用户就需要自己通过 delete 来释放资源, 这会明显增加内存泄漏的风险.另外值得注意的是, 不要试图用智能指针去管理动态分配的数组, 例如下面的代码:std::shared_ptr&amp;lt;int&amp;gt; ptr(new int[1024]);这行代码中试图用 shared_ptr 来管理一个包含 1024 个元素的整型数组, 这不会导致编译错误. 但是在 shared_ptr 等智能指针的析构函数中使用的是 delete 操作而非 delete[], 这会导致有一部分资源没有被释放. 如果在 new 表达式中使用了 [], 那么一定要在相应的 delete 表达式中也使用 []. 反之亦然.复制资源管理类时要小心由于在资源管理类中往往持有指向资源的指针, 因此在对其进行复制时需要谨慎应对. 通常我们应该考虑该资源是否能被复制, 该如何被复制, 这直接决定了资源管理类的复制方式. 常用的处理手段有如下几种: 禁止复制 采用引用计数法, 类似 shared_ptr 复制底部资源(深拷贝) 转移底部资源的所有权, 类似 unique_ptr将 newed 对象放入智能指针时应使用一条独立的语句考虑下面这两个函数, 前者可以为后者提供 prio 参数:int priority();void process_widget(std::shared_ptr&amp;lt;Widget&amp;gt; pw, int prio);如果我们用下面的方式来调用后者:process_widget(std::shared_ptr&amp;lt;Widget&amp;gt;(new Widget), priority());在调用该函数之前, 编译器首先要生成函数的实参, 这个过程中包含三个步骤: 执行 new Widget 调用 std::shared_ptr 构造函数 调用 priority() 函数而 C++ 以什么顺序来完成这三个步骤是不能确定的, 如果编译器采用了下面的方式: 执行 new Widget 调用 priority() 函数 调用 std::shared_ptr 构造函数在这种情况下, 一旦 priority() 函数中出现异常, 那么第一步中得到的指针将会遗失, 这将导致内存泄漏. 为了避免这种问题, 只需要将生成智能指针的部分代码分离成一条单独的语句即可:std::shared_ptr&amp;lt;Widget&amp;gt; pw(new Widget);process_widget(pw, priority());这样做是因为编译器只有在 同一条语句 内才有重新排列各项操作的自由." }, { "title": "构造-析构-赋值", "url": "/posts/%E6%9E%84%E9%80%A0-%E6%9E%90%E6%9E%84-%E8%B5%8B%E5%80%BC/", "categories": "Notes, C/C++", "tags": "Effective C++", "date": "2022-05-16 10:08:00 +0800", "snippet": "了解 C++ 默默编写并调用哪些函数C++ 能够自动生成默认的构造函数, 析构函数, 复制构造函数和赋值运算符. 这些函数都被声明为 public 和 inline, 且仅当它们需要被调用时才会被生成. 示例代码如下:Empty e1; // 生成默认构造函数Empty e2(e1); // 生成默认复制构造函数Empty e3 = e2; // 生成默认复制构造函数e1 = e3; // 生成默认赋值运算符而一旦在类中声明了一个构造函数, 无论其是否带有参数, 编译器都不会再为这个类生成默认构造函数.编译器生成的复制构造函数和赋值运算符中, 通常只负责把对象中的每一个 non-static 成员拷贝到目标对象中. 因此一旦类的成员中包含 引用或常量类型 等不可修改的成员变量时, 编译器将无法生成默认的函数.拒绝编译器自动生成函数如果需要实现一个不支持拷贝操作的类, 一种方法是将这两个函数声明为 private 并且不实现它们, 这样如果有人试图拷贝该类的对象时, 就会得到一个链接错误.而另一种方法是声明如下的一个基类 Uncopyable:class Uncopyable {protected: Uncopyable() {} ~Uncopyable() {}private: Uncopyable(const Uncopyable&amp;amp;); Uncopyable&amp;amp; operator=(const Uncopyable&amp;amp;);};这个基类允许子类生成其对象, 但一旦试图拷贝, 就会发生编译错误. 为了阻止某个类的对象被拷贝, 只需让该类继承 Uncopyable 类即可.关于虚析构函数 当类中包含至少一个虚函数时, 表明该类带有多态性质, 那么就 应该 为它声明虚析构函数; 而在其它情况下, 都 不应该 为它声明虚析构函数.在 C++ 中, 标准 string 类和 STL 中的容器类都不带有虚析构函数, 因此不能将其用于多态的用途, 更不应该试图继承它们.有时我们需要拥有一个抽象类, 但手上没有任何纯虚函数, 这时可以将析构函数声明为纯虚函数. 示例代码如下:class AWOV {public: virtual ~AWOV() = 0;};需要注意的是, 必须 为这个纯虚析构函数提供一个定义, 否则当其被调用时会出现链接错误.别让异常逃离析构函数应该尽可能避免在析构函数中抛出异常, 或是调用会抛出异常的函数. 而当这些情况不可避免时, 可以考虑下面的两种方法: 如果抛出异常, 就调用 std::abort 直接结束程序. 吞下发生的异常并进行记录, 之后让程序继续执行.这两个方法都不太靠谱, 前者直接杀死了本来或许还能继续运行的程序; 而后者则埋下了安全性隐患. 另一个策略是重新设计接口, 使得用户有机会来处理在析构过程中出现的异常, 例如下面的代码:class DBConn {public: ... void close() { db.close(); // 关闭数据库连接, 如果失败则抛出异常 closed = true; } ~DBConn() { if (!closed) { try { db.close(); } catch (...) { /* error log */ ... } } }private: DBConnection db; bool closed;};上面代码的核心思想在于, 用户能够在对象被析构之前主动调用 close() 函数来关闭连接, 并自行处理可能出现的异常; 而一旦用户没有这么做, 就意味着用户自己放弃了处理风险的机会, 那么将由析构函数来完成关闭连接的工作, 并吞下可能出现的异常.不要在构造和析构过程中调用虚函数考虑下面这个模拟股票交易的类和其子类的代码实现:class Transaction {public: Transaction(); virtual void log() const = 0; ...};Transaction::Transaction(){ ... log(); // 在基类的构造函数中调用了虚函数}class BuyTransaction: public Transaction {public: virtual void log() const; ...}这时, 我们创建一个 BuyTransaction 类的对象, 那么其基类 Transaction 的构造函数一定会首先被调用, 而当执行到 log() 函数时, 将会运行基类而非子类的版本, 这里由于该函数被声明为纯虚函数, 因此将会出现错误. 更加糟糕的一种情况是, 这个 log() 函数只是一般的虚函数, 并且在基类中进行了实现, 那么程序就将会直接执行基类的版本而不产生任何问题, 这可能造成潜在的隐患. 在子类对象的基类构造函数执行期间, 对象的类型将被 视为基类 . 对于析构函数来说同理.值得一提的是, 在上述情况中, 不止虚函数会被编译器解析到基类, 如果使用其他的运行期类型信息, 如 dynamic_cast 和 typeid, 也会将对象视为基类类型, 因此也不应该在构造/析构函数中使用它们.关于赋值运算符 operator= 函数应该返回一个对象自身的引用. 对于 += 这类的相关运算同样适用.这条原则主要是为了处理像 x = y = z 这样的连续赋值形式. 标准的赋值运算符形式如下:class Widget {public: ... Widget&amp;amp; operator=(const Widget&amp;amp; rhs) { ... return *this; }}我们还需要考虑对象 自我赋值 的情况, 下面的代码都有可能导致这种情况的发生:x = x;a[i] = a[j]; // i == j*pi = *pj; // pi, pj 指向同一个对象当自我赋值的对象 持有资源 时, 这将会导致严重的问题, 例如下面的代码:class Widget { ...private: int* p; // 指针, 指向一个从堆中分配的对象}Widget&amp;amp; Widget::operator=(const Widget&amp;amp; rhs){ delete p; p = new int(*rhs.p); return *this;}这个赋值操作看起来合理, 首先删除自身原来的指针, 并重新初始化得到一个资源的副本. 但在进行自我赋值时问题就会出现, 显然开始的 delete 操作释放了所持有的资源, 在下一步的代码中 rhs.p 就成为了一个野指针.能够解决这个问题的标准赋值运算代码如下:Widget&amp;amp; Widget::operator=(const Widget&amp;amp; rhs){ int* t = p; p = new int(*rhs.p); delete t; return *this;}上面代码的关键在于, 在复制之前不要删除. 这一点同样也保证了代码的 异常安全性, 即当 new 分配内存因为某些情况失败时, this-&amp;gt;p 不会变成野指针.复制对象时勿忘其每一个成分 复制函数应该确保复制 对象内的所有成员变量 以及 所有基类的成分 .当我们为一个子类编写复制函数时, 一定不要忘记复制其基类的成分, 否则编译器将会自动调用默认的复制函数, 这将可能导致隐藏的问题. 同时, 由于基类成分很多时候被继承为 private 而无法直接访问, 我们需要间接的调用基类的复制函数. 示例代码如下:Derive::Derive(const Derive&amp;amp; rhs) : Base(rhs) // 调用基类的复制构造函数{ ...}Derive&amp;amp; Derive::operator=(const Derive&amp;amp; rhs){ Base::operator=(rhs); // 调用基类的赋值运算符函数 ... return *this;} 不要 尝试用某个复制函数去实现另一个, 而应该将共同的部分放进第三个函数中, 并由两个复制函数共同调用.用赋值运算符函数调用复制构造函数是 不合理 的, 因为赋值运算符左边的对象是早已被构造完成的, 这时再调用复制构造函数, 相当于去构造一个已经存在的对象. 反之, 也不应该在复制构造函数中调用赋值运算符, 因为此时对象尚未构造完成." }, { "title": "从 C 到 C++", "url": "/posts/%E4%BB%8EC%E5%88%B0C++/", "categories": "Notes, C/C++", "tags": "Effective C++", "date": "2022-05-15 14:50:00 +0800", "snippet": "enum hack在 C++ 中, 大部分宏定义常量可以通过 const 关键字替换, 一个例外是在 class 编译期间需要使用一个 class 常量, 这种情况可以利用 enum 类型来实现替换. 其理论基础在于, 一个属于枚举类型的数值可以被视为 int 型来使用. 例如下面的类定义:class GamePlayer {private: enum { num_turns = 5 }; int scores[num_turns]; ...}; 与 #define 类似, 对 enum 取地址是不合法的. 如果 不想让别人获得一个指针或引用指向某个整数常量 , 可以通过 enum 来实现.使用 constconst_iterator迭代器的作用类似于 T*, 因此将迭代器声明为 const, 相当于声明一个 T* const 指针. 即指针不可变, 但指针所指的值是可变的. 如果需要一个 T const * 指针, 则需要使用 const_iterator.const 成员函数考虑下面这个用来表示一段文本的类:class TextBlock {public: ... const char&amp;amp; operator[](std::size_t pos) const; char&amp;amp; operator[](std::size_t pos);private: std::string text;};通过函数重载, 可以避免下面这类错误:const TextBlock ctb(&quot;word&quot;);ctb[0] = &#39;x&#39;; // 报错, 试图对一个 const 对象进行写操作Bitwise const 和 Logical const Bitwise const: 成员函数只有在不改变对象的任何成员变量( static 类型除外)时才可以说是 const. Logical const: 一个 const 类型成员函数可以在客户端侦测不出来的情况下修改对象的某些成员变量. 这需要通过 mutable 关键字实现. 示例代码如下:class TextBlock {public: ... std::size_t length() const;private: char * text; mutable std::size_t length; mutable bool length_valid;};std::size_t TextBlock::length() const{ if (!length_valid) { length = std::strlen(text); length_valid = true; } return length;}在 const 和 non-const 成员函数中避免重复同一个函数的 const 版本和 non-const 版本中往往包含大量的重合代码, 通常的做法是使这两个函数中的其中一个调用另一个, 这就需要使用 转型(casting) 进行常量性的变化. 示例代码如下:class TextBlock {public: ... const char&amp;amp; operator[](std::size_t pos) const { ... return text[pos]; } char&amp;amp; operator[](std::size_t pos) { return const_cast&amp;lt;char&amp;amp;&amp;gt;( static_cast&amp;lt;const TextBlock&amp;amp;&amp;gt;(*this)[pos] ); }};代码中需要使用两次转型, 首先要通过 static_cast 将 non-const 对象转型为 const 对象, 否则 [] 运算将会递归的调用自己. 在得到 const 成员函数的返回结果后, 再通过转型操作为其加上 const. 在 const 和 non-const 成员函数有实质等价的实现时, 可以令 non-const 版本调用 const 版本 , 而反向的做法是 不可取 的.确保对象在被使用前已被初始化区分赋值(assignment)和初始化(initialization)给出一个类 ABEntry, 代码如下:class ABEntry {public: ABEntry(const std::string&amp;amp; name, const std::string&amp;amp; addr);private: std::string name; std::string addr;};下面的代码在其构造函数中进行 赋值 :ABEntry::ABEntry(const std::string&amp;amp; name, const std::string&amp;amp; addr){ this-&amp;gt;name = name; this-&amp;gt;addr = addr; ...}而 初始化 通常发生在进入构造函数本体之前, 因此在上面的代码中, name 和 addr 其实发生了两次修改, 首先通过默认的构造函数进行了初始化, 之后才在函数体内被赋值. 这种做法导致了一次无效操作, 通常可以使用成员初值列来进行修改, 代码如下:ABEntry::ABEntry(const std::string&amp;amp; name, const std::string&amp;amp; addr) : name(name), addr(addr){ ...} C++ 有着固定的成员初始化次序, 基类 早于 子类被初始化, 而成员变量按照其 声明次序 被初始化.non-local static 对象的初始化static 对象包活 global 对象, 定义域 namespace 作用域内的对象, 在类内、在函数内以及在 file 作用域内被声明为 static 的对象. 函数内的 static 对象被称为 local static 对象, C++ 保证此类对象会在 函数被调用期间 被初始化. 而对于 non-local static 对象则没有这样的保证, 因此无法确定其会在何时被初始化. 例如下面的代码:class FileSystem { ... };FileSystem tfs;class Directory {public: Directory(); ...};Directory::Directory(){ ... std::size_t disks = tfs.numDisks(); ...}这时, 如果用户创建一个 Directory 对象, 而在构造函数中使用了尚未初始化的 tfs 对象, 就会导致错误出现. 解决此类问题的方法, 即是通过 reference-returning 函数 将 non-local static 对象替换为 local static 对象. 这也是 C++ 中单例(Singleton)模式的一个实现方法. 示例代码如下:class FileSystem { ... };FileSystem&amp;amp; tfs(){ static FileSystem fs; return fs;}在上面的代码中, 通过函数 tfs() 来替换原先的 tfs 对象, 在函数中初始化一个 local static 对象, 并返回一个引用指向该对象. 在多线程情况下, 这种做法仍存在不确定性, 一种可行的办法是在程序的单线程启动阶段手工调用所有 reference-returning 函数." }, { "title": "Python Mathematic Modules", "url": "/posts/Python-Mathematics-Modules/", "categories": "Notes, Python", "tags": "Python, Numpy, Matplotlib, Pandas", "date": "2018-08-20 00:00:00 +0800", "snippet": "numpyImportIt is common to import numpy under a briefer name np and also import some class frequently used like array. Common import as follows:import numpy as npfrom np import arrayArrayWays to create and some propertiesWe use array() function to generate an array object with a list as parameter. (The list can be multdimensional with nested [] operator.)The shape property of an array returns a tuple with the size of each array dimension.Sometimes we use np.random.randint() function to generate a random array. 3 arguments are required. First two represent range of random integer, the third arguments size= represents dimensionality(shape) of this array. As follows:a = np.random.randint(1, 8, size=(2, 3, 4))Besides, linspace() function can also generate an array. First two arguments of it is start and stop of a interval, at most cases we pass an integer representing numbers of elements in the array to be generated, and many other arguments are optional. If we let endpoint=True, then endpoints of the interval must be included in the new array.When used with an array, the len() function returns the length of the first axis.Methods to modifyArrays can be reshaped passing tuple that specify new dimensions as parameter to reshape() function. (Only one parentheses is also permitted.) Following code generates a $4\\times 1$ row vector.a = array([1, 2, 3, 4])a.reshape((2, 1))Notice that the reshape() function creates a new array rather than modifying the original one.One-dimensional versions of multi-dimensional arrays can be generated with flatten() function.Two or more arrays can be concatenated together using the concatenate() function with a tuple of the arrays to be joined. We can pass the second parameter represented a specified axis to which the arrays concatenate.Dimensionality of an array can be increased using newaxis constant in bracket notation as follows:a = array([1, 2, 3, 4])b = a[np.newaxis,:]c = a[:,np.newaxis]b.shape output is (1, 2) while c.shape output is (2, 1). It’s obvious that when newaxis is in front, a dimensionality added to front and when it is in the rear, a dimensionality added to rear. Similarly, if $a\\in\\mathbb{R}^{2\\times 3\\times 4} $, then we can use newaxis like a[:, np.newaxis, :, :], and a new dimensionality added to the corresponding position.IterationIt is possible to iterate over arrays in a manner similar to that of lists using for syntax.But for multidimensional arrays, iteration proceeds over the first axis such that each loop returns a subsection of the array. In such case, multiple assignment can also be used with array iteration as follows:a = np.array([[1, 2], [3, 4], [5, 6]])for (x, y) in a: print x * yOperationArray object owns many functions to implement basic array operations. Function Description Function Description sum() sum prod() product mean() average var() variance std() standard deviation median() median min() minimum max() maximum argmin() indices of minimum argmax() indices of maximum sort() sort unique() get unique elements For multidimensional arrays, each of the functions thus far described can take an optional argument axis that will perform an operation along only the specified axis, placing the results in a return arrayThere are many operations prepared for vactor and matrix mathematics in numpy. Add prefix numpy. to call them. Function Description Function Description inner() inner product outer() outer product dot() matrix product tensordot() tensor product diagonal() get diagonal elements transpose() transpose linalg.det() determinant linalg.inv() inverse linalg.pinv() pseudo-inverse linalg.norm() norm linalg.eig() eigenvalues and eigenvectors linalg.matrix_rank() rank linalg.solve() solve matrix equation linalg.svd() singular value decomposition matplotlibSome supplementary explanation to pyplot moduleMake a straight line on the vertical $x$ axisThis is a special syntax as follows:plot([x, x], [0, y])Add subplotWe call subplot() function to add a subplot to the figure. This function needs 3 integers $a, b, c$ as arguments, which means dividing figure to $a\\times b$ blocks, and the new plot will be added to the $c$-th block.To add several subplot, we can call subplots() function which return an array of each subplot. Pass the first and second argument as the row and column of the array respectively, and the third is usually keyword figsize=.Setting ticks and labelsIn x/yticks() function, we pass an array generated by np.linspace() as the first arguments. It’s worth mentioning that an list of str can perform as the second argument representing label of corresponding tick. LateX string is also permitted to be passed in a raw string.xticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi],[r&#39;$-\\pi$&#39;, r&#39;$-\\pi/2$&#39;, r&#39;$0$&#39;, r&#39;$+\\pi/2$&#39;, r&#39;$+\\pi$&#39;])Moving spinesWe can discard the top and right spines by setting their color to none and move the bottom and left ones to coordinate 0 in data space coordinates with code as follows:ax = gca()ax.spines[&#39;right&#39;].set_color(&#39;none&#39;)ax.spines[&#39;top&#39;].set_color(&#39;none&#39;)ax.xaxis.set_ticks_position(&#39;bottom&#39;)ax.spines[&#39;bottom&#39;].set_position((&#39;data&#39;,0))ax.yaxis.set_ticks_position(&#39;left&#39;)ax.spines[&#39;left&#39;].set_position((&#39;data&#39;,0))Adding a legendAdding a keyword argument label= to plot() function, and using legend() function with loc= as argument to add a legend to the specific plot.Use hist() function to generate a histogramFirstly, we pass an array as argument representing dataset. And some other optional keyword arguments are listed below: Argument Description bins= set number of bins density= If True, the first element of the return tuple will be the counts normalized to form a probability density color= set color of bins alpha= set transparency pandasATTENTION Pandas ! NOT panda !Read dataHere are some functions in pandas that are uesd to read data from different kind of file. Function Description read_csv() read CSV file read_json() read JSON file read_excel() read Excel file read_sql() read SQL database read_html() read URL, string or html file read_clipboard() read data from clipboard Dataframe() import data from Dataframe object Write dataHere are some functions in pandas that are uesd to write data into different kind of file. Function Description to_csv() write into CSV file to_josn() write into JSON file to_excel() write into Excel file to_sql() write into SQL database Dataframe and SeriesDataframe is the most important object of pandas which used to handle a large variety of files. We will discuss this object next.count() function returns number of samples in Dataframe.[] operator included name of one column can be used to get all elements in this column, and these elements are returned as a Series object, which is another important object of pandas.[] operator can include a bool expression as well. In code df[df[&#39;class&#39;] == &#39;Iris-setosa&#39;], we get all samples returned as a new Dataframe whose property class is equal to &#39;Iris-setosa&#39;.Another way to split data is using Dataframe object property ix with row and column index as arguments. [:] syntax is also available in this function, but when using this syntax, notice follow situation: code ix[:3, :2] gets datas in first 4 rows and first 2 columns.Further more, Python list iterator is also availble in ix property.Series object is a column in Dataframe, which usually used to describe one of prosperties of the sample. We can use unique() function to delete all repeat values in a single column and get unique values remaining returned as an array. As follows:df[&#39;class&#39;].unique()After getting the new Dataframe, we can use reset_index(drop=True) function to reset data index from 0, which makes it easier for further processing on these data.We call describe() function of Dataframe to get statistic data of the specific dataset. Output is similar as below: sepal length sepal width petal length petal widthcount 150.000000 150.000000 150.000000 150.000000mean 5.843333 3.054000 3.758667 1.198667std 0.828066 0.433594 1.764420 0.763161min 4.300000 2.000000 1.000000 0.10000025% 5.100000 2.800000 1.600000 0.30000050% 5.800000 3.000000 4.350000 1.30000075% 6.400000 3.300000 5.100000 1.800000max 7.900000 4.400000 6.900000 2.500000In addition, call corr() function to show the correspondence between properties of samples in dataset." }, { "title": "Radial Basic Function Network", "url": "/posts/Radial-Basic-Function-Network/", "categories": "Notes, Machine Learning", "tags": "ML, RBF, K-Means, 算法", "date": "2018-08-19 00:00:00 +0800", "snippet": "RBF Network LearningFirstly we consider the Gaussian SVM which map our data to a infinite-dimensional space\\[g_\\text{SVM}(\\boldsymbol x) = \\text{sign}\\left(\\sum_\\text{SV}\\alpha_ny_n\\exp(-\\gamma\\Vert\\boldsymbol x-\\boldsymbol x_n\\Vert^2)+b\\right) \\tag{12.1}\\]here Gaussian kernel is also called Radial Basis Function(RBF) and radial means this model only depends on distance between $\\boldsymbol x$ and ‘center’ $\\boldsymbol x_n$.Let $g_n(\\boldsymbol x)=y_n\\exp(-\\gamma\\Vert\\boldsymbol x-\\boldsymbol x_n\\Vert^2)$, then $(12.1)$ is rewritten as\\[g_\\text{SVM}(\\boldsymbol x) = \\text{sign}\\left(\\sum_\\text{SV}\\alpha_ng_n(\\boldsymbol x)+b\\right) \\tag{12.2}\\]it can be referred as a neural network whose hidden layer consists of those ‘center’ points and output layer is simple linear aggregationin a general form, RBF network hypothesis is\\[h(\\boldsymbol x)=\\text{Output}\\left(\\sum_\\text{m=1}^M\\beta_m\\text{RBF}(\\boldsymbol x, \\boldsymbol \\mu_m)+b\\right) \\tag{12.3}\\]and the key variables in this model is each center $\\boldsymbol\\mu_m$, and in SVM that is the support vectors.In order to determine the center $\\boldsymbol\\mu_m$, we firstly consider a simple situation called full RBF network, where we consider all the points in the dataset as the center, that is we set $M$ simply equal to $N$. And we know the Gaussian function decrease at a very fast rate, so the final summation tends to be decided by the nearest neighbor $\\boldsymbol\\mu_m$ of the input $\\boldsymbol x$, that is\\[g_\\text{nbor}(\\boldsymbol x)=y_m \\ \\ \\ \\ \\ \\text{such that } \\boldsymbol x \\text{ closest to } \\boldsymbol x_m \\tag{12.4}\\]further, we can also choose $k$ neighbors to aggregate which is called $k$ nearest neighbor.Next we try to use squared error regression to calculate the optimal $\\beta_m$ in full RBF Network instead of the lazy method mentioned above. Similar to linear regression, we rewrite the hypothesis as follows\\[h(\\boldsymbol x)=\\sum_\\text{n=1}^n\\beta_n\\text{RBF}(\\boldsymbol x, \\boldsymbol x_n) \\tag{12.5}\\]record $\\boldsymbol z_n = (\\text{RBF}(\\boldsymbol x_n, \\boldsymbol x_1),\\text{RBF}(\\boldsymbol x,_n \\boldsymbol x_2),…,\\text{RBF}(\\boldsymbol x_n, \\boldsymbol x_N))^\\text T$, and according to the derivation in linear regression, we can similarly get the optimal $\\boldsymbol\\beta$\\[\\boldsymbol \\beta =(\\boldsymbol Z^\\text T\\boldsymbol Z)^{-1}\\boldsymbol Z^\\text T\\boldsymbol y \\tag{12.6}\\]another hand, here $\\boldsymbol Z$ is a symmetric square matrix, then $(12.6)$ can be reduced to\\[\\boldsymbol \\beta=\\boldsymbol Z^{-1}\\boldsymbol y \\tag{12.7}\\]and use this $\\boldsymbol \\beta$ we find the inner error rate is $0$, obviously when we apply this model to action, it may cause serious overfit. So we commonly use ridge regression for $\\boldsymbol \\beta$ instead.\\[\\boldsymbol \\beta =(\\boldsymbol Z^\\text T\\boldsymbol Z+\\lambda\\boldsymbol I)^{-1}\\boldsymbol Z^\\text T\\boldsymbol y \\tag{12.8}\\]Another way to regularize is choosing some points in the whole set as prototypes, and then consider these points as centers $\\boldsymbol \\mu_m$.k-Means AlgorithmNext we try to cluster with the prototype $\\boldsymbol\\mu_m$, divide the whole dataset into $M$ disjoint sets $S_1, S_2, … , S_M$ and choose $\\boldsymbol\\mu_m$ for each $S_m$. For each $\\boldsymbol x_i, \\boldsymbol x_j$ both belong to $S_m$, we hope that $\\boldsymbol \\mu_m \\approx \\boldsymbol x_i \\approx \\boldsymbol x_j$.The squared error is\\[E_\\text{in}(S_1,S_2,...,S_M;\\boldsymbol\\mu_1,\\boldsymbol\\mu_2,...,\\boldsymbol\\mu_M)=\\frac{1}{N}\\sum_{n=1}^N\\sum_{m=1}^M[\\![\\boldsymbol x_n \\in S_m]\\!]\\Vert\\boldsymbol x_n-\\boldsymbol \\mu_m\\Vert^2 \\tag{12.9}\\]this problem is hard to optimize, we consider $\\lbrace S_m\\rbrace_{m=1}^M, \\lbrace\\boldsymbol \\mu_m\\rbrace_{m=1}^M$ as two sets of variables and then optimize them respectively. When $S_1,S_2,…,S_M$ is fixed, we derivative $E_\\text{in}$ on $\\boldsymbol\\mu_m$\\[\\begin{split}\\nabla_{\\boldsymbol\\mu_m}E_\\text{in} &amp;amp;= -2\\sum_{n=1}^N[\\![\\boldsymbol x_n \\in S_m]\\!](\\boldsymbol x_n-\\boldsymbol\\mu_m) \\\\[1em]&amp;amp;=-2\\left(\\sum_{\\boldsymbol x_n \\in S_m}\\boldsymbol x_n-|S_m|\\boldsymbol \\mu_m\\right)\\end{split}\\tag{12.10}\\]then set $\\nabla_{\\boldsymbol\\mu_m}E_\\text{in}=0$, we will find the optimal $\\boldsymbol \\mu_m$ is just the average of $\\boldsymbol x_n$ within $S_m$. When $\\boldsymbol\\mu_1,\\boldsymbol\\mu_2,…,\\boldsymbol\\mu_M$ is fixed, we optimize $S_m$ by divide each $\\boldsymbol x_n$ into the cluster with the closest $\\boldsymbol\\mu_m$.Finally, we get the $k$-Means Algorithm, in which we first choose $k$ $\\boldsymbol x_n$ randomly as $\\boldsymbol\\mu_1,\\boldsymbol\\mu_2,…\\boldsymbol\\mu_k$ and then keep optimizing $S_1,S_2,…,S_k$ and $\\boldsymbol\\mu_1,\\boldsymbol\\mu_2,…,\\boldsymbol\\mu_k$ until converge." }, { "title": "Neural Network", "url": "/posts/Neural-Network/", "categories": "Notes, Machine Learning", "tags": "ML, 神经网络, 算法", "date": "2018-08-15 00:00:00 +0800", "snippet": "Neural Network is a classic algorithm originated from perceptron. Linearly combine various perceptrons such as the figurethe final hypothesis $G$ is\\[G(\\boldsymbol x)=\\text{sign}\\left(\\sum_{t=1}^T\\alpha_t\\underbrace{\\text{sign}(\\boldsymbol w^\\text T_t\\boldsymbol x)}_{g_t(\\boldsymbol x)}\\right) \\tag{11.1}\\]it is a neural network of two layers, and $\\boldsymbol\\alpha$ and $\\boldsymbol w_t$ is weights of each layer. Through adding more layers wa can make the model more powerful. Another hand, we replace $\\text{sign}$ function with $\\tanh$ function which has good mathematical properties.\\[\\tanh(x) = \\frac{\\exp(x)-\\exp(-x)}{\\exp(x)+\\exp(-x)} \\tag{11.2}\\]Consider a more complex neural network with $L$ layersthe input $\\boldsymbol x$ can be regarded as the $0$-th layer, then record numbers of nodes in each layer as $d^{(0)}, d^{(1)}, … , d^{(L)}$, and $w_{ij}^{(\\ell)}$ is weight in $\\ell$-th layer from $i$-th node in $(\\ell-1)$-th layer to $j$-th node in $\\ell$-th layer, $s_j^{(\\ell)}$ is score of $j$-th node in $\\ell$-th layer, $x_i^{(\\ell)}$ is transformed input of $i$-th node in $\\ell$-th layer.\\[\\begin{align}s_j^{(\\ell)} &amp;amp;= \\sum_{i=0}^{d^(\\ell-1)}w_{ij}^{(\\ell)}x_i^{(\\ell-1)} \\tag{11.3} \\\\[1em]x_j^{(\\ell)} &amp;amp;=\\begin{cases}\\tanh(s_j^{(\\ell)}) &amp;amp; \\ell &amp;lt; L \\\\[1em]s_j^{(l)} &amp;amp; \\ell=L\\end{cases}\\tag{11.4}\\end{align}\\]Next we try to learn the optimal weight. Firstly consider the output layer, error of $\\boldsymbol x_n$ is\\[\\begin{split}E_n &amp;amp;= \\left(y_n-s_1^{(L)}\\right)^2 \\\\[1em]&amp;amp;= \\left(y_n-\\sum_{i=0}^{d^{(L-1)}}w_{i1}^{(L)}x_i^{(L-1)}\\right)^2\\end{split}\\tag{11.5}\\]we have\\[\\begin{split}\\frac{\\partial E_n}{\\partial w_{i1}^{(L)}} &amp;amp;= \\frac{\\partial E_n}{\\partial s_1^{(L)}} \\cdot \\frac{\\partial s_1^{(L)}}{\\partial w_{i1}^{(L)}} \\\\[1em]&amp;amp;= -2\\left(y_n-s_1^{(L)}\\right) \\cdot x_i^{(L-1)}\\end{split}\\tag{11.6}\\]record $\\delta_1^{(L)}=-2\\left(y_n-s_1^{(L)}\\right)$, similarly we can get derivation in general $\\ell$-th layer\\[\\begin{split}\\frac{\\partial E_n}{\\partial w_{ij}^{(\\ell)}} &amp;amp;= \\frac{\\partial E_n}{\\partial s_j^{(\\ell)}} \\cdot \\frac{\\partial s_j^{(\\ell)}}{\\partial w_{ij}^{(\\ell)}} \\\\[1em]&amp;amp;= \\delta_j^{(\\ell)} \\cdot x_i^{(\\ell-1)}\\end{split}\\tag{11.7}\\]next we try to get $\\delta_j^{(\\ell)}$. To get $E_n$ from $s_j^{(\\ell)}$, we have the following process\\[s_j^{(\\ell)} \\mathop{\\Longrightarrow}^{\\tanh} x_j^{(\\ell)} \\mathop{\\Longrightarrow}^{w_{jk}^{(\\ell+1)}} \\left(\\begin{array}{ccc}s_1^{(\\ell+1)} \\\\[0.5em]\\vdots \\\\[0.5em]s_k^{(\\ell+1)} \\\\[0.5em]\\vdots\\end{array} \\right) \\Longrightarrow \\cdots \\Longrightarrow E_n \\tag{11.8}\\]use chain rule to get the derivation\\[\\begin{split}\\delta_j^{(\\ell)} &amp;amp;= \\frac{\\partial E_n}{\\partial s_j^{(\\ell)}} = \\sum_{k=1}^{d^{(\\ell+1)}} \\frac{\\partial E_n}{\\partial s_k^{(\\ell+1)}} \\cdot \\frac{\\partial s_k^{(\\ell+1)}} {\\partial x_j^{(\\ell)}} \\cdot \\frac{\\partial x_j^{(\\ell)}}{\\partial s_j^{(\\ell)}} \\\\[1em]&amp;amp;= \\sum_{k=1}^{d^{(\\ell+1)}} \\delta_k^{(\\ell+1)} \\cdot w_{jk}^{(\\ell+1)} \\cdot \\tanh&#39;\\left(s_j^{(\\ell)}\\right)\\end{split}\\tag{11.9}\\]from $(11.9)$ we get the recursive formula of $\\delta^{(\\ell)}$ and $\\delta^{(\\ell+1)}$, and based on it we have Backpropagation Algorithm.AlgorithmInitialize all weights $w_{ij}^{(\\ell)}$for $t=0,1,…,T$ stochastic: randomly pick $n \\in {1,2,…,N}$ forward: compute all $x_i^{(\\ell)}$ with $\\boldsymbol x^{(0)}=\\boldsymbol x_n$ backward: compute all $\\delta_j^{(\\ell)}$ subject to $\\boldsymbol x^{(0)}=\\boldsymbol x_n$ gradient descent: $w_{ij}^{(\\ell)} \\gets w_{ij}^{(\\ell)}-\\eta\\delta_j^{(\\ell)}x_i^{(\\ell-1)}$return" }, { "title": "Adaptive Boosting", "url": "/posts/Adaptive-Boosting/", "categories": "Notes, Machine Learning", "tags": "ML, Boosting, 算法", "date": "2018-08-13 00:00:00 +0800", "snippet": "AdaBoost is an algorithm that promotes weak learners to a strong learner. This algorithm first trains a base learner from the initial training set, and then adjusts the sample distribution according to its performance, so that samples where previous learner makes mistake will receive more attention later. Then train the next learner based on the adjusted sample distribution. Repeatedly, and get the final learner by weighting all the learners.With a parameter $u_n^{(t)}$ represents the degree to which sample $\\boldsymbol x_n$ is concerned in $t$-th iteration. Firstly we can assume all $u_n^{(1)} = \\frac{1}{N}$. Our goal is to minimize the traning error\\[E_{\\text{in}} = \\frac{1}{N}\\sum_{n=1}^Nu_n\\cdot \\text{err}(y_n,h(\\boldsymbol x_n)) \\tag{10.1}\\]each iteration to find the optimal $g$ can be done with SVM or logistic regression and so on. If we use $0/1$ error in $(10.1)$, then the optimal $g$ can be written as\\[g_t= \\ \\mathop{\\arg\\min}_{h\\in \\mathcal H}\\sum_{n=1}^Nu_n^{(t)} [\\![y_n \\ne h(\\boldsymbol x)]\\!] \\tag{10.2}\\]after finding a $g_t$, we adjust $u_n$ to get $g_{t+1}$. In the process, we make $u_n$ of samples $\\boldsymbol x_n$ where $g_t$ makes mistake large as possible. Record the error rate of $g_t$ as $\\epsilon_t$, we construct degree factor\\[\\text{deg}_t=\\sqrt{\\frac{1-\\epsilon_t}{\\epsilon_t}} \\tag{10.3}\\]for each correct $\\boldsymbol x_n$, we set $u_n^{t+1} \\gets \\frac{1}{\\text{deg}_t} u_n^t$, while for each error $\\boldsymbol x_n$, we set $u_n^{t+1} \\gets \\text{deg}_t \\cdot u_n^t$. Next we look at the degree factor, when $0&amp;lt;\\epsilon_t&amp;lt;\\frac{1}{2}$, which means $g$ works not bad in training set, and in this condition we have $\\text{deg}_t&amp;gt;1$. Through updating $u_n$, we scale up error points and scale down correct points. When $\\epsilon_t=\\frac{1}{2}$, which means $g$ is so bad with a correct rate same as coin flipping, then we find $\\text{deg}_n=1$, so it does nothing when update $u_n$. Finally when $\\frac{1}{2}&amp;lt;\\epsilon_t&amp;lt;1$, it looks so terrible, but if we ‘reserve’ this learner, that is consider samples $g(\\boldsymbol x_n)=1$ as negative while samples $g(\\boldsymbol x_n)=-1$ as positive, then its correct error is $1-\\epsilon_t$, which seems not too bad. In this condition we have $0&amp;lt;\\text{deg}_t&amp;lt;1$, through updating $u_n$, we scale up correct points (actually wrong points) and scale down error points (actually correct points).In addition, our final hypotheis\\[G(\\boldsymbol x) = \\text{sign}\\left(\\sum_{t=1}^T\\alpha_tg_t(\\boldsymbol x)\\right) \\tag{10.4}\\]where $\\alpha_t$ can be also got during each iteration, we set\\[\\alpha_t = \\ln\\sqrt{\\frac{1-\\epsilon_t}{\\epsilon_t}} \\tag{10.5}\\]reason for choosing this $\\alpha_t$ is similar to the degree factor, which used to describe the contribution of each learner.In theory, we have a conclusion that if each learner is better than coin flipping, only after $T=O(\\log N)$ iteration we can reduce $E_{\\text{in}}(G)$ approximately equal to $0$.In action, a popular choice of each $g$ is ‘decision stump’, that is\\[h_{s,i,\\theta}(\\boldsymbol x)=s\\cdot \\text{sign}(x_i-\\theta) \\tag{10.6}\\]where $s,i,\\theta$ represent direction, feature and threshold respectively. This algorithm makes a positive and negative rays on $i$-th feature dimension each time. Perhaps this algorithm is too weak to work by itself, but after AdaBoost, it can be really strong." }, { "title": "Support Vector Machine 2", "url": "/posts/Support-Vector-Machine-2/", "categories": "Notes, Machine Learning", "tags": "ML, SVM, 支持向量机, 分类, 回归, 算法", "date": "2018-08-10 00:00:00 +0800", "snippet": "Soft-Margin SVMIn order to avoid the effect of noise in dataset, we allow the model to make a small number of mistakes, similar to PLA, we have a new optimization goal shaped like\\[\\begin{split}\\min_{b,\\boldsymbol w} \\ \\ &amp;amp;\\frac{1}{2}\\boldsymbol w^\\text T\\boldsymbol w+C \\cdot \\sum_{n=1}^N[\\![y_n \\ne \\text{sign}(\\boldsymbol w^\\text T\\boldsymbol x+b)]\\!] \\\\[1em]\\text{s.t.} \\ \\ &amp;amp;y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)\\ge 1-\\infty \\ \\cdot [\\![y_n \\ne \\text{sign}(\\boldsymbol w^\\text T\\boldsymbol x+b)]\\!], \\ n=1,2,...,N\\end{split}\\tag{9.31}\\]the constraint means we only care about points separated correctly, for these error point we take them into the penalty in our goal and use constant $C$ to set our tolerance for errors. However, there are two cons of our model. Firstly it is non-linear, and secondly the penalty of this model cannot distinguish points with small error or large error. In order to solve the two problems, we modify our goal as follows:\\[\\begin{split}\\min_{b,\\boldsymbol w,\\boldsymbol\\xi} \\ \\ &amp;amp;\\frac{1}{2}\\boldsymbol w^\\text T\\boldsymbol w+C \\cdot \\sum_{n=1}^N\\xi_n \\\\[1em]\\text{s.t.} \\ \\ &amp;amp;y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)\\ge 1-\\xi_n \\\\[1em]&amp;amp;\\xi_n \\ge 0, \\ n=1,2,...,N\\end{split}\\tag{9.32}\\]where $\\xi_n$ describes the ‘margin violation’ , in other words the degree of error in sample point.Next we construct Lagrange dual problem similar as before. With an increased constraints, we set two Lagrange multipliers $\\alpha_n$ and $\\beta_n$. Then we have Lagrange function\\[\\mathcal L(b,\\boldsymbol w,\\boldsymbol\\xi,\\boldsymbol\\alpha,\\boldsymbol\\beta) = \\frac{1}{2}\\boldsymbol w^\\text T\\boldsymbol w+C \\cdot \\sum_{n=1}^N\\xi_n + \\sum_{n=1}^N\\alpha_n\\left(1-\\xi_n-y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)\\right) + \\sum_{n=1}^N\\beta_n(-\\xi_n) \\tag{9.33}\\]and the dual problem is\\[\\max_{\\alpha_n \\ge 0, \\beta_n \\ge 0} \\ \\min_{b,\\boldsymbol w,\\boldsymbol\\xi} \\ \\ \\mathcal L(b,\\boldsymbol w,\\boldsymbol\\xi,\\boldsymbol\\alpha,\\boldsymbol\\beta) \\tag{9.34}\\]Firstly, we set $\\nabla_{\\xi_n}\\mathcal L = 0$, we have\\[C-\\alpha_n-\\beta_n=0 \\tag{9.35}\\]which means we can eliminate $\\beta_n$ with $C-\\alpha_n$, and add a new constraint $0 \\le \\alpha_n \\le C$.\\[\\begin{split}\\mathcal L(b,\\boldsymbol w,\\boldsymbol\\xi,\\boldsymbol\\alpha,\\boldsymbol\\beta) &amp;amp;= \\frac{1}{2}\\boldsymbol w^\\text T\\boldsymbol w+C \\cdot \\sum_{n=1}^N\\xi_n - \\sum_{n=1}^N\\alpha_n\\xi_n + \\sum_{n=1}^N\\alpha_n\\left(1-y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)\\right) + \\sum_{n=1}^N(C-\\alpha_n)(-\\xi_n) \\\\[1em]&amp;amp;=\\frac{1}{2}\\boldsymbol w^\\text T\\boldsymbol w + \\sum_{n=1}^N\\alpha_n\\left(1-y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)\\right)\\end{split} \\tag{9.36}\\]and we find $\\xi_n$ is also eliminated in this transform, the problem has become very close to that in hard-margin SVM. After derivation, we can get a similar form as follows:\\[\\begin{split}&amp;amp;\\min_{\\boldsymbol \\alpha} \\ \\frac{1}{2}\\sum_{n=1}^N\\sum_{m=1}^N\\alpha_n\\alpha_my_ny_m\\boldsymbol x_n^\\text T\\boldsymbol x_m - \\sum_{n=1}^N \\alpha_n \\\\[1em]&amp;amp;\\text{s.t.}\\begin{cases}0 \\le \\alpha_n \\le C \\\\[1em]\\sum_{n=1}^N\\alpha_ny_n=0 \\\\[1em]\\boldsymbol w=\\sum_{n=1}^N\\alpha_ny_n\\boldsymbol x_n, \\ \\beta_n=C-\\alpha_n \\\\[1em]\\end{cases}\\end{split}\\tag{9.37}\\]As we did in hard-margin SVM, we have constraints $\\alpha_n\\left(1-\\xi_n-y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)\\right)=0$ and $(C-\\alpha_n)\\xi_n=0$. We try to find a ‘free’ support vector $\\boldsymbol x_s$ such that $0&amp;lt;\\alpha_s&amp;lt;C$, we get\\[\\begin{split}b &amp;amp;= y_s-\\boldsymbol w^\\text T\\boldsymbol x_s \\\\[1em]&amp;amp;=y_s-\\sum_{n=1}^N\\alpha_ny_nK(\\boldsymbol x_n,\\boldsymbol x_s)\\end{split}\\tag{9.38}\\]According to the specific $\\alpha_n$, we can judge the position of sample $\\boldsymbol x_n$. when $\\alpha_n=0$, then $\\xi_n=0$. The sample is non support vector which should be away from the boundary. when $0&amp;lt;\\alpha_n&amp;lt;C$, then $\\xi_n=0$. The sample is a free support vector located on the boundary. (square in figure) when $\\alpha_n=C$, then $\\xi_n&amp;gt;0$ describes violation amount. The sample violates the boundary. (triangle in figure)At the same time, we find that $\\xi_n=1-y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)$ if $\\boldsymbol x_n$ violates the margin and else $\\xi_n=0$. In a brief form, we have\\[\\xi_n=\\max\\left(1-y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b),0\\right) \\tag{9.39}\\]according to this formula, we can rewrite our objective in $(9.32)$ as follows:\\[\\min_{b,\\boldsymbol w} \\ \\ \\frac{1}{2}\\boldsymbol w^\\text T\\boldsymbol w+C \\cdot \\sum_{n=1}^N\\max\\left(1-y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b),0\\right) \\tag{9.40}\\]Consider the first item in this formula as L-2 regularizer and the second item describes the degree of error, then the soft-margin SVM can be classified as a kind of a unique form of regularization.Kernel Logistic RegressionKernel tricks help us save expense of calculating in high dimension space, it can also be applied into logistic regression like a ‘two-level learning’.With feature transformation $\\boldsymbol\\Phi(\\boldsymbol x)$, the optimal hypothesis of logistic regression is written as\\[g(\\boldsymbol x) = \\sigma(\\boldsymbol w^\\text T\\boldsymbol\\Phi(\\boldsymbol x)+b) \\tag{9.41}\\]according to previous derivation, we have\\[\\boldsymbol w^\\text T\\boldsymbol\\Phi(\\boldsymbol x)+b=\\sum_{n=1}^N\\alpha_ny_nK_{\\boldsymbol\\Phi}(\\boldsymbol x_n,\\boldsymbol x)+y_s-\\sum_{n=1}^N\\alpha_ny_nK_{\\boldsymbol\\Phi}(\\boldsymbol x_n,\\boldsymbol x_s) \\tag{9.42}\\]view $\\boldsymbol w^\\text T\\boldsymbol\\Phi(\\boldsymbol x)+b$ as a special transform, recorded as $\\Phi_\\text{SVM}(\\boldsymbol x)$, which can easily calculted with kernel tricks, then $g$ becomes\\[g(\\boldsymbol x)=\\sigma\\left(A\\cdot\\Phi_\\text{SVM}(\\boldsymbol x)+B\\right) \\tag{9.43}\\]where $A$ describes scaling and $B$ describes shifting, then the optimization goal in logistic regression becomes\\[\\min_{A,B} \\ \\frac{1}{N}\\sum_{n=1}^N\\ln\\left(1+\\exp\\left(-y_n(A\\cdot\\Phi_\\text{SVM}(\\boldsymbol x)+B\\right)\\right) \\tag{9.44}\\]then the problem has reduced to a 1st-dim logistic regression with only two variables.This method finding a transform $\\Phi_\\text{SVM}(\\boldsymbol x)$ in a high space and return a scalar value, next we try to do exact logistic regression in the high space with kernel. Consider L-2 regularized logistic regression, the optimization goal is\\[\\min_{\\boldsymbol w} \\ \\frac{1}{N}\\sum_{n=1}^N\\ln(1+ \\exp (-y_n\\boldsymbol w^{\\text T}\\boldsymbol x_n)) + \\frac{\\lambda}{N}\\boldsymbol w^\\text T\\boldsymbol w \\tag{9.45}\\]from which we can ensure the final $\\boldsymbol w$ is the linear combination of $\\boldsymbol x_n$, and we can further prove it established for each L-2 regularized linear model\\[\\min_{\\boldsymbol w} \\ \\frac{\\lambda}{N}\\boldsymbol w^\\text T\\boldsymbol w+\\frac{1}{N}\\sum_{n=1}^N\\text{err}(y_n, \\boldsymbol w^\\text T\\boldsymbol x) \\tag{9.46}\\]the optimal $\\boldsymbol w$ can be written as $\\boldsymbol w_\\ast=\\sum_{n=1}^N\\beta_n\\boldsymbol x_n$. Assume we have a optimal $\\boldsymbol w_\\ast$ violates this equality, and $\\boldsymbol w_\\ast=\\boldsymbol w_\\bot+\\boldsymbol w_\\Vert$, where $\\boldsymbol w_\\bot \\in \\text{ span}(\\boldsymbol x_n)$ and $\\boldsymbol w_\\Vert \\bot \\text{ span}(\\boldsymbol x_n)$. then we have\\[\\text{err}(y_n, \\boldsymbol w_*^\\text T\\boldsymbol x)=\\text{err}(y_n, (\\boldsymbol w_\\bot+\\boldsymbol w_\\Vert)^\\text T\\boldsymbol x) = \\text{err}(y_n, \\boldsymbol w_\\Vert^\\text T\\boldsymbol x) \\tag{9.47}\\]another hand\\[\\begin{split}\\boldsymbol w_*^\\text T\\boldsymbol w_* &amp;amp;= \\boldsymbol w_\\bot^\\text T\\boldsymbol w_\\bot + 2\\boldsymbol w_\\bot^\\text T\\boldsymbol w_\\Vert + \\boldsymbol w_\\Vert^\\text T\\boldsymbol w_\\Vert \\\\[1em]&amp;amp;= \\boldsymbol w_\\bot^\\text T\\boldsymbol w_\\bot + \\boldsymbol w_\\Vert^\\text T\\boldsymbol w_\\Vert \\\\[1em]&amp;amp;\\ge \\boldsymbol w_\\Vert^\\text T\\boldsymbol w_\\Vert\\end{split}\\tag{9.48}\\]so we have a better $\\boldsymbol w_\\Vert$ more optimal than $\\boldsymbol w_*$ which is a contradiction.With this condition, substitute the optimal $\\boldsymbol w_*=\\sum_{i=1}^N\\beta_i\\boldsymbol x_i$ into the goal function, we have\\[\\begin{split}\\boldsymbol w^\\text T\\boldsymbol x_n &amp;amp;= \\sum_{i=1}^N\\beta_i\\boldsymbol x_i^\\text T\\boldsymbol x_n = \\sum_{n=1}^N\\beta_iK(\\boldsymbol x_i,\\boldsymbol x_n) \\\\[1em]\\boldsymbol w^\\text T\\boldsymbol w &amp;amp;= \\sum_{i=1}^N\\beta_i\\boldsymbol x_i^\\text T \\cdot \\sum_{i=1}^N\\beta_i\\boldsymbol x_i \\\\[1em]&amp;amp;= \\sum_{i=1}^N\\sum_{j=1}^N\\beta_i\\beta_jK(\\boldsymbol x_i,\\boldsymbol x_j)\\end{split}\\tag{9.49}\\]then the problem becomes finding a optimal $\\boldsymbol\\beta$ instead of $\\boldsymbol w$\\[\\min_{\\boldsymbol \\beta} \\ \\frac{1}{N}\\sum_{n=1}^N\\ln\\left(1+ \\exp \\left(-y_n\\sum_{n=1}^N\\beta_iK(\\boldsymbol x_i,\\boldsymbol x_n)\\right)\\right) + \\frac{\\lambda}{N}\\sum_{i=1}^N\\sum_{j=1}^N\\beta_i\\beta_jK(\\boldsymbol x_i,\\boldsymbol x_j)\\tag{9.50}\\]this form is called Kernel Logistic Regression, easily find it is quite similar with SVM.Support Vector RegressionIn general rigde regression, we measure the error in training dataset with square error $(y_n-\\boldsymbol w^\\text T\\boldsymbol x_n)^2$, according to our derivation above, it can be rewritten with kernel as $\\left(y_n-\\sum_{i=1}^N\\beta_iK(\\boldsymbol x_i,\\boldsymbol x_n)\\right)^2$. Then our optimization goal is\\[\\min_{\\boldsymbol \\beta} \\ \\frac{1}{N}\\sum_{n=1}^N\\left(y_n-\\sum_{i=1}^N\\beta_iK(\\boldsymbol x_i,\\boldsymbol x_n)\\right)^2 + \\frac{\\lambda}{N}\\sum_{i=1}^N\\sum_{j=1}^N\\beta_i\\beta_jK(\\boldsymbol x_i,\\boldsymbol x_j)\\tag{9.51}\\]this complicated formula can be simplified with matrix form as follows:\\[\\begin{split}&amp;amp;\\min_{\\boldsymbol\\beta} \\ \\frac{1}{N}\\Vert\\boldsymbol y-\\boldsymbol {K\\beta}\\Vert + \\frac{\\lambda}{N}\\boldsymbol\\beta^\\text T\\boldsymbol{K\\beta} \\\\[1em]=&amp;amp; \\min_{\\boldsymbol\\beta} \\frac{1}{N}\\left(\\boldsymbol\\beta^\\text T\\boldsymbol K^\\text T\\boldsymbol{K\\beta}-2\\boldsymbol\\beta^\\text T\\boldsymbol K^\\text T\\boldsymbol y + \\boldsymbol y^\\text T\\boldsymbol y \\right) + \\frac{\\lambda}{N}\\boldsymbol\\beta^\\text T\\boldsymbol{K\\beta}\\end{split}\\tag{9.52}\\]record this objective as $E_\\text{aug}$ and then differentiate $\\boldsymbol\\beta$, we have\\[\\begin{split}\\nabla_{\\boldsymbol\\beta}E_\\text{aug} &amp;amp;= \\frac{2}{N}\\left(\\lambda\\boldsymbol {K\\beta}+\\boldsymbol K^\\text T\\boldsymbol {K\\beta}-\\boldsymbol K^\\text T\\boldsymbol y\\right) \\\\[1em]&amp;amp;= \\frac{2}{N}\\boldsymbol K^\\text T\\left((\\lambda\\boldsymbol I+\\boldsymbol K)\\boldsymbol\\beta-\\boldsymbol y\\right)\\end{split}\\tag{9.53}\\]set $\\nabla_{\\boldsymbol\\beta}E_\\text{aug}=0$, we have one analytic solution\\[\\boldsymbol\\beta=(\\lambda\\boldsymbol I+\\boldsymbol K)^{-1}\\boldsymbol y \\tag{9.54}\\]this inverse matrix always exists because we have $\\lambda&amp;gt;0$ and $\\boldsymbol K$ is positive semi-definite. Complexity of train with kernel regression is $O(N^3)$, which is quite high for big data, and in that case, linear regression may be a good alternatives.Similar to soft-margin SVM, we consider a special ‘tube’ regressionas shown in above image, points in the tube is regarded as no error while points outside the tube have a error measured by distance to tube. That is\\[\\text{err}(y,\\hat y)=\\begin{cases}0 &amp;amp; |\\hat y-y| \\le \\epsilon \\\\[1em]|\\hat y-y|-\\epsilon &amp;amp; |\\hat y-y| &amp;gt; \\epsilon\\end{cases}\\tag{9.55}\\]where $\\hat y=\\boldsymbol w^\\text T\\boldsymbol x_n+b$, in general we have\\[\\text{err}(y, \\hat y)= \\max(0,|\\hat y-y|-\\epsilon) \\tag{9.56}\\]this form of error is called $\\epsilon$-insentive Error with $\\epsilon&amp;gt;0$. Then our optimization goal is\\[\\min_{b,\\boldsymbol w} \\ C\\sum_{n=1}^N\\max(0, \\vert\\hat y_n-y_n\\vert - \\epsilon)+\\frac{1}{2}\\boldsymbol w^\\text T\\boldsymbol w \\tag{9.57}\\]the $\\max$ function is non-differentiable in some point, we introduce a slack variable $\\xi_n = \\max(0, \\vert \\hat{y}_n - y_n \\vert - \\epsilon)$ similar to soft-margin SVM, and rewrite $(9.57)$\\[\\begin{split}\\min_{b,\\boldsymbol w,\\boldsymbol\\xi} \\ &amp;amp;C\\sum_{n=1}^N\\xi_n+\\frac{1}{2}\\boldsymbol w^\\text T\\boldsymbol w \\\\[1em]\\text{s.t.} \\ &amp;amp;|\\hat y_n-y_n| \\le \\epsilon+\\xi_n \\\\[1em]&amp;amp; \\xi_n \\ge 0\\end{split}\\tag{9.58}\\]in this formula, when $\\vert\\hat y_n-y_n\\vert-\\epsilon \\le 0$, in order to minimize the objective, it should have $\\xi_n=0$, and when $\\vert\\hat y_n-y_n\\vert - \\epsilon &amp;gt; 0$, then $\\xi_n \\ge \\vert\\hat y_n-y_n\\vert-\\epsilon$ according to the first constraint, to minimize the objective, we have $\\xi_n = \\vert\\hat y_n-y_n\\vert-\\epsilon$. This shows that introducing this variable is reasonable. Further, remove absolute value and transform the constraint into linear form\\[\\begin{split}\\min_{b,\\boldsymbol w,\\boldsymbol\\xi^&amp;lt;,\\boldsymbol\\xi^&amp;gt;} \\ &amp;amp;C\\sum_{n=1}^N(\\xi_n^&amp;lt;+\\xi_n^&amp;gt;)+\\frac{1}{2}\\boldsymbol w^\\text T\\boldsymbol w \\\\[1em]\\text{s.t.} \\ &amp;amp;\\boldsymbol w^\\text T\\boldsymbol x_n+b-y_n \\le \\epsilon+\\xi_n^&amp;gt; \\\\[1em]&amp;amp;\\boldsymbol w^\\text T\\boldsymbol x_n+b-y_n \\ge -\\epsilon-\\xi_n^&amp;lt; \\\\[1em]&amp;amp; \\xi_n^&amp;lt; \\ge 0, \\ \\xi_n^&amp;gt; \\ge 0\\end{split}\\tag{9.59}\\]where $\\xi_n^&amp;lt;$ is the lower tube violations while $\\xi_n^&amp;gt;$ is the upper tube violations. Introduce Lagrange multipliers $\\alpha^&amp;lt;_n$ and $\\alpha^&amp;gt;_n$, then do some calculations as before, we can have some of the KKT conditions:\\[\\begin{align}\\boldsymbol w &amp;amp;= \\sum_{n=1}^N(\\alpha_n^&amp;lt;-\\alpha_n^&amp;gt;)\\boldsymbol x_n \\tag{9.60} \\\\[1em]0 &amp;amp;= \\sum_{n=1}^N(\\alpha_n^&amp;lt;-\\alpha_n^&amp;gt;) \\tag{9.61} \\\\[1em]0 &amp;amp;= \\alpha_n^&amp;lt;(\\boldsymbol w^\\text T\\boldsymbol x_n+b-y_n+\\epsilon+\\xi_n^&amp;lt;) \\tag{9.62} \\\\[1em]0 &amp;amp;= \\alpha_n^&amp;gt;(\\boldsymbol w^\\text T\\boldsymbol x_n+b-y_n-\\epsilon-\\xi_n^&amp;gt;) \\tag{9.63} \\\\[1em]\\end{align}\\]similarly, we can get the dual probblem of SVR, which can be solved with QP computing program. According to formula $(9.62)$ and $(9.63)$, we have the following two situations: For points within tube, we have\\[\\begin{split}&amp;amp;|\\boldsymbol w^\\text T\\boldsymbol x_n+b-y_n| \\le \\epsilon \\\\[1em]\\Rightarrow \\ &amp;amp; \\xi_n^&amp;lt;=0, \\ \\xi_n^&amp;gt;=0 \\\\[1em]\\Rightarrow \\ &amp;amp; \\boldsymbol w^\\text T\\boldsymbol x_n+b-y_n+\\epsilon+\\xi_n^&amp;lt; \\ne 0, \\ \\boldsymbol w^\\text T\\boldsymbol x_n+b-y_n-\\epsilon-\\xi_n^&amp;gt; \\ne 0 \\\\[1em]\\Rightarrow \\ &amp;amp;\\alpha_n^&amp;lt;=0, \\ \\alpha_n^&amp;gt;=0\\end{split}\\tag{9.64}\\] clearly, these points make no contribution to the final $\\boldsymbol w$. For points outside tube, which are support vectors in this model, we can calculate $\\boldsymbol w$ and $b$ through these points. " }, { "title": "Python基础", "url": "/posts/Python%E5%9F%BA%E7%A1%80/", "categories": "Notes, Python", "tags": "Python", "date": "2018-08-08 00:00:00 +0800", "snippet": "正则表达式在Python中使用正则表达式 用 import re 导入正则表达式模块. 用 re.compile() 函数创建一个 Regex 对象 (使用原始字符串 r&#39;&#39;) . 调用 Regex 对象的 search() 方法传入想要查找的字符串, 并返回一个 Match 对象. 调用 Match 对象的 group() 方法, 返回实际匹配文本的字符串.import rephoneNumRegex = re.compile(r&#39;\\d{3}-\\d{3}-\\d{4}&#39;)searchStr = input(&#39;Enter: &#39;)mo = phoneNumRegex.search(searchStr)if not mo: print(&#39;NO MATCH!&#39;)else: print(&#39;phone number found: &#39;, mo.group())修改正则表达式为 (\\d{3})-(\\d{3}-\\d{4}) , 第一对括号对应 group(1) , 第二对括号对应 group(2) .向 re.compile() 函数传入第二个参数 re.IGNORECASE 或 re.I , 使正则表达式不区分大小写.常用的匹配 Regex Match Batman|Tina Fey Batman or Tina Fey Bat(man|women) Batman or Batwomen Bat(wo)?man words before &#39;?&#39; is optionalmatchs same as above Bat{3} BatBatBat Bat{3,5} matches word ‘Bat’ repeated 3-5 timese.g. ‘hahahahaha’ we get ‘hahahahaha’ Bat{3,5}? similar to above but matches word as few as possiblee.g. ‘hahahahaha’ we get ‘hahaha’ Bat{3,}Bat{,5} matches word ‘Bat’ repeated more than 2 timesmatchs word ‘Bat’ repeated less than 6 times 用 sub() 方法替换字符串Regex 对象的 sub() 方法有两个参数, 第一个参数是用来替代的字符串, 第二个是待匹配的字符串.import renamesRegex = re.compile(r&#39;Agent \\w+&#39;)stg = &#39;Agent Alice gave the secret documents to Agent Bob&#39;stg = namesRegex.sub(&#39;Unknown&#39;, stg)print(stg)可在第一个参数中用 \\1, \\2... 表示用匹配得到的分组 (Group) 1, 2…的文本进行替换.import renamesRegex = re.compile(r&#39;Agent (\\w)\\w*&#39;)stg = &#39;Agent Alice gave the secret documents to Agent Bob&#39;stg = namesRegex.sub(r&#39;\\1****&#39;, stg)print(stg)方法 findall() 与 search() 之间的区别 findall() 方法返回 list 类型, 每个 list item 为 tuple 类型, 由匹配的所有组构成. e.g. 以匹配邮箱的正则表达式 (([a-zA-Z0-9._%+-]+)@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}) 对字符串 &#39;824658314@qq.com&#39; 进行匹配, 结果为 [(&#39;824658314@qq.com&#39;, &#39;824658314&#39;)] . search() 方法返回 match 类型, 通过 match 对象的 group() 方法来获得匹配的各个组. 另由 groups() 方法获得匹配的所有组. e.g. 同上例, 由 group() 得 824658314@qq.com , group(1) 亦得 824658314@qq.com , group(2) 得 824658314 . 文件读写os.path 模块中的常用函数 函数名 描述 join() 将参数合成为一个路径字符串 abspath() 相对路径转化绝对路径 isabs() 判断是否为绝对路径 relpath() 有两个参数 path 和 start , 返回从 path 到 start 的路径字符串 dirname() 返回路径的目录名称 basename() 返回路径的基本名称(文件名) split() 返回路径目录名称和基本名称的 tuple getsize() 返回文件的字节数 exists() 判断文件或文件夹是否存在 os 模块中的常用函数 函数名 描述 makedirs() 创建新文件夹 getcwd() 获得当前工作目录 chdir() 改变当前工作目录 listdir() 返回当前文件夹中所有文件名的 list unlink() 删除目标文件 rmdir() 删除目标文件夹,该文件夹需为空 调试异常 raise 语句常出现在一个函数中, 以抛出异常. 其包含以下部分: raise 关键字. 对 Exception() 函数的调用. 将包含出错信息的字符串传给 Exception() 函数. raise Exception(&#39;This is the error message.&#39;) try 和 except 语句常在调用函数的代码中用来处理异常. try: func(x)except Exception as err: print(&#39;An exception happened: &#39; + str(err)) 断言断言语句包含以下部分: assert 关键字. 条件 (求值为 True 或 False 的表达式). 逗号. 当条件为 False 时显示的字符串.assert judgement == True, &#39;Error!&#39;日志将下面的代码添加到程序顶部, 即可在程序运行时输出日志信息.import logginglogging.basicConfig(level=logging.DEBUG, format=&#39; %(asctime)s - %(levelname)s - %(message)s&#39;)其中的参数 level 为日志级别, 有如下选项: 级别 描述 DEBUG 最低级别, 用于小细节. INFO 记录程序中的一般事件, 或确认工作一切正常 WARNING 表示可能的问题, 但不会阻止程序工作 ERROR 记录错误, 将导致程序执行某些任务时失败 CRITICAL 最高级别, 表示致命错误, 将导致程序终止 通过调用函数 logging.disable(level) 函数, 将禁止 level 级别及更低级别的所有日志信息.向 logging.basicConfig() 函数中传入参数 filename=&#39;...&#39; 以将日志保存在指定文件中.从 Web 抓取信息用 request 模块下载文件通过调用 request 模块中 get() 函数, 得到对应 URL 的 Reponse 对象.Response 对象有如下常用成员: 成员名 描述 text 下载的页面作为字符串存储于该成员中 status_code 反映对网页的请求是否成功 raise_for_status() 当下载文件出错时抛出异常 iter_content() 返回包含指定字节的一段内容 下面的代码从指定 URL 下载文件并保存到硬盘:import requestsres = requests.get(&#39;http://www.gutenberg.org/cache/epub/1112/pg1112.txt&#39;)res.raise_for_status()txtFile = open(&#39;/Users/sameal/Desktop/1.txt&#39;, &#39;wb&#39;)for chunk in res.iter_content(100000): txtFile.write(chunk)txtFile.close()用 BeautifulSoup 模块解析 HTML创建 bs4 对象的两种方法: 利用 request.get() 方法下载页面, 再将 Response 对象的 text 属性传给 bs4.BeautifulSoup() 函数, 并携带第二个参数为 html.parser. 直接向 bs4.BeautifulSoup() 函数传递一个 HTML 文件, 并携带第二个参数为 html.parser.用 bs4 解析 HTML 的步骤: 调用 request.get() 方法获取目标页面的 Response 对象. 创建目标页面的 bs4 对象. 调用 bs4 对象的 select() 方法, 获取指定内容的 list, 其中每个元素为一个 Tag 对象. 其中 select() 方法的参数为 CSS 选择器的组合. 如下代码将匹配一个 &amp;lt;a&amp;gt; 标签, 这个标签直接包含在 id 为 page 的容器中, 该容器直接包含于符合类 row-fluid 及 page-wrap 的容器中. soup.select(&#39;.row-fluid.page-wrap &amp;gt; #page &amp;gt; a&#39;) 通过 Tag 对象的 get() 方法, 传入的参数为 Tag 对象的一个属性名, 如 href, src 等. 处理 Excel 表格读取openpyxl 模块在最近更新中废弃了大量 get 方法, 改为直接获取属性值.导入 openpyxl 模块后, 调用 openpyxl.load_workbook() 函数, 返回一个代表当前 Excel 文件的 Workbook 对象, 其有如下成员: 成员名 描述 sheetnames 获取所有工作表的名字`` [] 返回指定工作表 active 返回默认工作表 [:] 返回指定区域内的的所有单元格 rows 返回所有行的 list columns 返回所有列的 list 每个工作表由一个 Worksheet 对象表示, 用 [] 运算符能直接获得各个单元格的 Cell 对象, 也可通过调用方法 cell(row=, column=) 获得.Cell 对象有如下成员: 成员名 描述 value 获取单元格的值 row 获取行数 column 获取列数 coordinate 获取坐标 写入通过调用 openpyxl.Workbook() 方法可以获得一个新的工作簿.Workbook 对象中有一些用于写入数据的函数: 函数名 描述 create_sheet(index=, title=) 添加工作表 remove() 删除指定工作表传入参数为 Worksheet 类型, 而非名字 另外, 通过直接对属性赋值也能够起到修改工作表的作用.在修改完成之后, 调用 Workbook 对象的 save() 方法将工作表保存在指定路径.处理 Word 文档在 python-docx 中, doc 文件用三种类型表示. Document 对象表示整个文档, 其包含一个 Paragraph 对象的列表, 表示文档中的段落. 每个 Paragraph 对象都包含一个 Run 对象的列表, 表示相同样式的文本. 上述三个对象均可由上级对象的属性获取.若要向 doc 文件中写入数据, 则调用 docx.Document() 方法, 创建空白的 Document 对象. 其包含如下成员函数: 函数名 描述 add_paragraph() 添加新的段落 add_heading() 添加标题, 传入第二个参数 0-4 代表标题的层次 add_picture() 在文档末添加图像, 可选参数 width= 和 height= 调用 Paragraph 对象的 add_run() 方法, 可添加不同格式的文本.调用 Run 对象的 add_break() 方法, 可添加换行符. 若传入参数 docx.text.WD_BREAK.PAGE , 可添加换页符.另外, 通过 Paragraph 和 Run 对象的 style 属性, 可改变文本样式.处理 CSV 文件Python 中内置用来处理 CSV 文件的模块 csv, 在程序开头导入即可.将 CSV 文件路径作参数传给 csv.reader() 函数, 得到与文件相关联的 Reader 对象. 将 Reader 对象传递给 next() 函数, 返回一个 list, 其中元素为文件下一行各列的数据. 将上述列表作参数传给 enumerate() 函数, 返回该行中各列元素索引及其值构成的 tuple.如下代码获得 CSV 文件中首行各元素的索引及值:import csvwith open(&#39;pcc-master/chapter_16/sitka_weather_07-2014.csv&#39;) as csvFile: reader = csv.reader(csvFile) header_row = next(reader) for idx, column_header in enumerate(header_row): print(idx, column_header)遍历 Reader 对象可访问 CSV 文件各行, 通过 [] 运算符可得到该行各列元素.处理 JSON 文件Python 中内置用来处理 JSON 文件的模块 json, 在程序开头导入即可.将 JSON 文件路径作为参数传给 json.load() 函数, 即可获得文件中所有 dict 值构成的 list. 之后按照对 dict 的处理方式即可方便的对 JSON 文件进行处理.数据可视化pyplot 模块导入 pyplot 模块的代码:import matplotlib.pyplot as pltpyplot 模块中的函数: 函数名 描述 figure() 生成空白画布 plot() 传入两个 list 对象, 绘制对应的折线图在空白画布上 scatter() 传入一组坐标或两个 list 对象,生成对应的散点图 show() 显示绘制的图象 x/ylabel() 设置 $x/y$ 轴的名称 x/ylim() 设置 $x/y$ 轴标度的最大值和最小值 x/yticks() 设置 $x/y$ 轴标度的范围和步长, 传入 np.linspace tick_params() 设置坐标轴刻度的样式 title() 设置标题 axis() 传入包含四个元素的 list , 设置坐标轴的取值范围 savefig() 保存生成的图象 上述函数中还能传入其他参数: 参数名 描述 figsize= 设置图形的尺寸, 传入一个 tuple linewidth= 设置绘制线条的宽度 linestyle= 设置线条样式, 如&#39;-&#39;,&#39;-.&#39; 等 s= 设置绘制散点的尺寸 color= 设置散点的颜色 cmap= 配合 color= 使用颜色映射 fontsize= 设置字号 edgecolor= 设置数据点边缘色 更详细的内容在 Mathematics Module 板块中.pygal 模块pygal 模块中的成员: 成员名 描述 Bar() 创建条形图对象 _title 设置或取得图表的标题 x/y_labels 设置或取得 $x$ 轴和 $y$ 轴的标度 _x/y_title 设置或取得 $x$ 轴和 $y$ 轴的名称 add() 传入指定值的标签和指定值的列表, 填充到条形图中 render_to_file 将图表保存到文件 处理日期datetime 模块中 datetime 类的 strptime() 方法, 能够解析日期字符串, 其第一个参数为 str 对象, 第二个参数有多个可选值: paraName Description %A name of week, e.g. Monday %B name of month, e.g. January %m month expressed by number(01-12) %d day expressed by number(01-31) %Y 4-digit year, e.g. 2018 %y 2-digit years e.g. 18 %H hour expressed in 24-hour format %I hour expressed in 12-hour format %p am or pm %M minute (00-59) %S second (00-59) " }, { "title": "Support Vector Machine 1", "url": "/posts/Support-Vector-Machine-1/", "categories": "Notes, Machine Learning", "tags": "ML, SVM, 支持向量机, 分类, 算法", "date": "2018-08-06 00:00:00 +0800", "snippet": "Linear SVMLarge-Margin ProblemFirstly, we consider 3 linear classifier on the same dataset as follows:both of them seem performing well but the rightmost one whose hyperplane is farthest from samples is likely to be better.Assume a Gaussian-like noise on future sample $\\boldsymbol x \\approx \\boldsymbol x_n$ in gray area of the following figure:if $\\boldsymbol x_n$ is further from hyperplane, that is, distance between hyperplane and the closest $\\boldsymbol x_n$ is greater, then the classifier can tolerate more noise and is more robust to overfitting.This distance mentioned above is called Margin, then our goal is finding a $\\boldsymbol w$ which classifies samples correctly and has the maximum margin at the same time, that is\\[\\begin{split}\\max_{\\boldsymbol w} \\ \\ &amp;amp;\\text{margin}(\\boldsymbol w,b) \\\\[1em]\\text{s.t.} \\ \\ &amp;amp;y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b) &amp;gt; 0, \\ n=1,2,...,N \\\\[1em]&amp;amp;\\text{margin}(\\boldsymbol w,b)=\\min_{n=1,2,...,N}\\text{distance}(\\boldsymbol x_n, \\boldsymbol w, b)\\end{split}\\tag{9.1}\\]where $\\boldsymbol x_n=(x_1,x_2,…,x_d)^\\text T$, $\\boldsymbol w=(w_1,w_2,…,w_d)^\\text{T}$, $b$ represents bias and hypothesis $h(\\boldsymbol x)=\\text{sign}(\\boldsymbol w^\\text T\\boldsymbol x+b)$, which is little different from that in perceptron.Distance in $(9.1)$ can be written as\\[\\text{distance}(\\boldsymbol x,\\boldsymbol w,b)=\\frac{|\\boldsymbol w^\\text T\\boldsymbol x+b|}{\\Vert\\boldsymbol w\\Vert} \\tag{9.2}\\]considering for every $n$, $y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)&amp;gt;0$, we have\\[\\text{distance}(\\boldsymbol x_n,\\boldsymbol w,b)=\\frac{y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)}{\\Vert\\boldsymbol w\\Vert} \\tag{9.3}\\]For a specific hyperplane, scaling the coefficients of each items at the same times makes no difference. In the origin model, hyperplane equation is $\\boldsymbol w^\\text T\\boldsymbol x+b=0$, we assume $\\boldsymbol w^\\text T\\boldsymbol x_n+b=k$. In order to make this form simpler, rewrite hyperplane equation as $\\frac{1}{k}(\\boldsymbol w^\\text T\\boldsymbol x+b)=0$, then set $ \\tilde{\\boldsymbol w}=\\frac{1}{k}\\boldsymbol w$ and $\\tilde b=\\frac{1}{k}b$, so we have $\\tilde{\\boldsymbol w}^\\text T\\boldsymbol x_n+\\tilde b=1$. According to the derivation above, we set\\[\\min_{n=1,2,...,N}y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)=1 \\tag{9.4}\\]then we have $\\text{margin}(\\boldsymbol w,b)=\\frac{1}{\\Vert\\boldsymbol w\\Vert}$ and further we can ensure that for every $n$, $y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b) &amp;gt; 0$ under $(9.4)$.Therefore, our goal has become\\[\\begin{split}\\max_{\\boldsymbol w,b} \\ \\ &amp;amp;\\frac{1}{\\Vert\\boldsymbol w\\Vert} \\\\[1em]\\text{s.t.} \\ \\ &amp;amp;\\min_{n=1,2,...,N}y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)=1\\end{split}\\tag{9.5}\\]However, this problem is still not easy enough to solve. Next we consider to replace this constraint with its necessary constraint $y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)\\ge1, \\ n=1,2,…,N$. To prove the feasibility of this replacement, assume we find the optimal solution $(\\boldsymbol w, b)$ when $ y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)&amp;gt;k$ and $k&amp;gt;1$. Then we set $\\tilde{\\boldsymbol w}=\\frac{\\boldsymbol w}{k}$ and $\\tilde b=\\frac{b}{k}$, obviously the new $(\\tilde{\\boldsymbol w}, \\tilde b)$ is more optimal, so the contradiction is found.After some conversion, we have the particular standard problem\\[\\begin{split}\\min_{\\boldsymbol w,b} \\ \\ &amp;amp;\\frac{1}{2}\\boldsymbol w^\\text T\\boldsymbol w \\\\[1em]\\text{s.t.} \\ \\ &amp;amp;y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)\\ge1, \\ n=1,2,...,N\\end{split}\\tag{9.6}\\]we call each boundary sample Support Vector, which satisfies $y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)=1$. And from previous derivation, we know the above optimization problem to be solved only depends on these support vectors.Quadratic ProgrammingFortunately, our goal is a quadratic problem which is easy to solve, next we try to convert our goal to a standard form of quadratic programming $\\text{QP}(\\boldsymbol Q,\\boldsymbol p,\\boldsymbol A,\\boldsymbol c)$:\\[\\begin{split}\\min_{\\boldsymbol u} \\ \\ &amp;amp;\\frac{1}{2}\\boldsymbol u^\\text T\\boldsymbol Q\\boldsymbol u+\\boldsymbol p^\\text{T} \\boldsymbol u \\\\[1em]\\text{s.t.} \\ \\ &amp;amp;\\boldsymbol a_m^\\text T\\boldsymbol u \\ge c_m, \\ m=1,2,...,M\\end{split}\\tag{9.7}\\]Compare this standard form, we have\\[\\begin{align}\\boldsymbol u &amp;amp;= (b;\\boldsymbol w) \\\\[1em]\\boldsymbol Q &amp;amp;= \\left(\\begin{array}{ccc}0 &amp;amp; \\boldsymbol 0_d^\\text T \\\\\\boldsymbol 0_d &amp;amp; \\boldsymbol I_d\\end{array}\\right) \\\\[1em]\\boldsymbol p &amp;amp;= \\boldsymbol 0_{d+1} \\\\[1em]\\boldsymbol a_m^\\text T &amp;amp;= y_m(1, \\boldsymbol x_n^\\text T) \\\\[1em]c_m &amp;amp;= 1 \\\\[1em]M &amp;amp;= N\\end{align}\\tag{9.8}\\]then the optimal solution can be obtained by passing these parameters into a function calculating the quadratic programming.In addition, when the dataset is non-linear separable, we can similarly use a feature transformation $\\boldsymbol\\Phi$ as it is in linear regression. Only thing we need to do is replace $\\boldsymbol x$ in above formula with $\\boldsymbol z=\\boldsymbol\\Phi(\\boldsymbol x)$.DualityAll above derivation we have done is based on the condition where the feature dimension $d$ is not very large, but after feature transformation, the new $\\tilde d$ of $\\boldsymbol z$ may be quite large or even infinite, then challenges arise at this time. To avoid calculating in a high dimension, we try to find the Dual Problem of original SVM.One of our key tool is lagrange multiplier which we have mentioned in regularization. From $(9.6)$, we construct the Lagrange function as follows:\\[\\mathcal L(b,\\boldsymbol w, \\boldsymbol\\alpha) = \\frac{1}{2}\\underbrace{\\boldsymbol w^\\text T\\boldsymbol w}_{\\text{objective}} + \\sum_{n=1}^N\\alpha_n(\\underbrace{1-y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)}_{\\text{constraint}}) \\tag{9.9}\\]where all $\\alpha_n \\ge 0$. For each $(b,\\boldsymbol w)$ satisfy the constraint in $(9.9)$, we have $1-y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b) \\le 0$, which means $\\mathcal L$ gets maximum value only when all $\\alpha_n(1-y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b))=0$. Therefore, we have $\\max\\mathcal L(b,\\boldsymbol w,\\boldsymbol\\alpha)=\\frac{1}{2}\\boldsymbol w^\\text T\\boldsymbol w$. For $(b, \\boldsymbol w)$ violate the constraint, the maximum value of $\\mathcal L$ can be infinite when $\\alpha_n \\to \\infty$. On this condition, if we take the minimum on $\\max\\mathcal L(b,\\boldsymbol w,\\boldsymbol\\alpha)$, that is\\[\\min_{b,\\boldsymbol w}\\left(\\max_{\\alpha_n \\ge 0}\\mathcal L(b,\\boldsymbol w,\\boldsymbol\\alpha) \\right) \\tag{9.10}\\]we can ensure the optimal $(b,\\boldsymbol w)$ of $(9.10)$ meet all constraints.For any fixed $\\boldsymbol\\alpha’$ with all $\\alpha_n’ \\ge 0$, we have\\[\\max_{\\alpha_n \\ge 0}\\mathcal L(b,\\boldsymbol w,\\boldsymbol\\alpha) \\ge \\mathcal L(b,\\boldsymbol w,\\boldsymbol\\alpha&#39;) \\tag{9.11}\\]and then\\[\\min_{b,\\boldsymbol w}\\left(\\max_{\\alpha_n \\ge 0}\\mathcal L(b,\\boldsymbol w,\\boldsymbol\\alpha) \\right) \\ge \\min_{b,\\boldsymbol w}\\mathcal L(b, \\boldsymbol w,\\boldsymbol\\alpha&#39;) \\tag{9.12}\\]considering the arbitrariness of $\\boldsymbol\\alpha’$, $(9.12)$ established for all $\\boldsymbol\\alpha$, so we further have\\[\\min_{b,\\boldsymbol w}\\left(\\max_{\\alpha_n \\ge 0}\\mathcal L(b,\\boldsymbol w,\\boldsymbol\\alpha) \\right) \\ge \\max_{\\alpha_n\\ge0}\\left(\\min_{b,\\boldsymbol w}\\mathcal L(b, \\boldsymbol w,\\boldsymbol\\alpha)\\right) \\tag{9.13}\\]which we call Lagrange Dual Problem. if the dual problem solved, we get a lower bound of the origin problem. And in this problem it is a Strong Duality which means both sides of the inequality is exactly equivalent. Then our goal has become\\[\\max_{\\alpha_n\\ge0} \\ \\min_{b,\\boldsymbol w}\\left(\\frac{1}{2}\\boldsymbol w^\\text T\\boldsymbol w + \\sum_{n=1}^N\\alpha_n(1-y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)\\right) \\tag{9.14}\\]Firstly the inner problem, set $\\nabla_b\\mathcal L=0$, we get\\[\\sum_{n=1}^N\\alpha_ny_n=0\\tag{9.15}\\]then our goal is reduced to\\[\\max_{\\alpha_n\\ge0} \\ \\min_{\\boldsymbol w}\\left(\\frac{1}{2}\\boldsymbol w^\\text T\\boldsymbol w + \\sum_{n=1}^N\\alpha_n(1-y_n\\boldsymbol w^\\text T\\boldsymbol x_n)\\right) \\tag{9.16}\\]next set $\\nabla_{\\boldsymbol w}\\mathcal L=0$, we get\\[\\boldsymbol w=\\sum_{n=1}^N\\alpha_ny_n\\boldsymbol x_n \\tag{9.17}\\]rewrite our goal\\[\\begin{split}&amp;amp;\\max_{\\alpha_n\\ge0} \\left(-\\frac{1}{2}\\boldsymbol w^\\text T\\boldsymbol w + \\sum_{n=1}^N\\alpha_n\\right) \\\\[1em]=&amp;amp;\\max_{\\alpha_n\\ge0} \\left(-\\frac{1}{2}\\Vert\\sum_{n=1}^N\\alpha_ny_n\\boldsymbol x_n\\Vert^2 + \\sum_{n=1}^N\\alpha_n\\right) \\\\[1em]=&amp;amp; \\min_{\\alpha_n\\ge0} \\left(\\frac{1}{2}\\Vert\\sum_{n=1}^N\\alpha_ny_n\\boldsymbol x_n\\Vert^2 - \\sum_{n=1}^N\\alpha_n\\right) \\\\[1em]\\end{split}\\tag{9.18}\\]then we have standard dual SVM\\[\\begin{split}&amp;amp;\\min_{\\boldsymbol \\alpha} \\ \\frac{1}{2}\\sum_{n=1}^N\\sum_{m=1}^N\\alpha_n\\alpha_my_ny_m\\boldsymbol x_n^\\text T\\boldsymbol x_m - \\sum_{n=1}^N \\alpha_n \\\\[1em]&amp;amp;\\text{s.t.}\\begin{cases}\\alpha_n\\ge0 \\\\[1em]\\alpha_n(1-y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b))=0 \\\\[1em]\\sum_{n=1}^N\\alpha_ny_n=0 \\\\[1em]\\boldsymbol w=\\sum_{n=1}^N\\alpha_ny_n\\boldsymbol x_n\\end{cases}\\end{split}\\tag{9.19}\\]constraints listed above is called KKT Conditions. Dual SVM is also a quadratic programming, we can get the optimal $\\boldsymbol\\alpha$ by rewriting the object and constrints to the standard form and then passing it into the QP function. Aftering get the optimal $\\boldsymbol\\alpha$, we can easily get the optimal $\\boldsymbol w$ using constraint $(9.17)$, and according to $\\alpha_n(1-y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b))=0$, if we find a $\\alpha_n$ grater than $0$, then $b$ can be obtained by solving $1-y_n(\\boldsymbol w^\\text T\\boldsymbol x_n+b)=0$. From this we find the model is only determined by the support vector.KernelWhen we use feature transformation $\\boldsymbol z = \\boldsymbol\\Phi(\\boldsymbol x)$ to solve the problem in a high space $\\tilde d$, then the complexity to calculate $\\boldsymbol z_n^\\text T\\boldsymbol z_m$ is $O(\\tilde d)$. It costs a lot when $\\tilde d$ is quite large. We are tring to find a method to get rid of the effect of $\\tilde d$.Firstly consider a $2$nd order polynomial transform\\[\\boldsymbol\\Phi_2(\\boldsymbol x)=(1,x_1,x_2,...,x_d,x_1^2,x_1x_2,...,x_2x_1,x_2^2,x_2x_3,...,x_2x_d,...,x_d^2) \\tag{9.20}\\]then\\[\\begin{split}\\boldsymbol\\Phi_2^\\text T(\\boldsymbol x)\\boldsymbol\\Phi_2(\\boldsymbol x&#39;)&amp;amp;=1+\\sum_{i=1}^dx_ix_i&#39;+\\sum_{i=1}^d\\sum_{j=1}^dx_ix_jx_i&#39;x_j&#39; \\\\[1em]&amp;amp;=1+\\sum_{i=1}^dx_ix_i&#39;+\\sum_{i=1}^dx_ix_i&#39;\\sum_{j=1}^dx_jx_j&#39; \\\\[1em]&amp;amp;=1+\\boldsymbol x^\\text T\\boldsymbol x&#39;+(\\boldsymbol x^\\text T\\boldsymbol x&#39;)(\\boldsymbol x^\\text T\\boldsymbol x&#39;)\\end{split}\\tag{9.21}\\]The complexity of calculating inner product in $\\boldsymbol z$ space is $O(d^2)$, but through $(9.21)$, we reduced it to $O(d)$. Record $K_{\\boldsymbol\\Phi}(\\boldsymbol x, \\boldsymbol x’)=\\boldsymbol\\Phi(\\boldsymbol x)^\\text T\\boldsymbol\\Phi(\\boldsymbol x’)$, called Kernel Function.With kernel function, the optimization target can be rewrote as\\[\\min_{\\boldsymbol\\alpha} \\ \\frac{1}{2}\\sum_{n=1}^N\\sum_{m=1}^N\\alpha_n\\alpha_my_ny_mK(\\boldsymbol x_n,\\boldsymbol x_m) - \\sum_{n=1}^N \\alpha_n \\tag{9.22}\\]as mentioned above, we get $b=y_s-\\boldsymbol w^\\text T\\boldsymbol x_s$ from the support vector, another hand we have $\\boldsymbol w=\\sum_{n=1}^N\\alpha_ny_n\\boldsymbol x_n $ in $(9.17)$, therefore we rewrite $b$ as following form with kernel:\\[\\begin{split}b &amp;amp;= y_s-\\sum_{n=1}^N\\alpha_ny_n\\boldsymbol x_n^\\text T\\boldsymbol x_s \\\\[1em]&amp;amp;= y_s-\\sum_{n=1}^N\\alpha_ny_nK(\\boldsymbol x_n,\\boldsymbol x_s)\\end{split}\\tag{9.23}\\]besides, the final hypothesis of SVM is $g_\\text{SVM}(\\boldsymbol x)=\\text{sign}(\\boldsymbol w^\\text T\\boldsymbol x+b)$, also we can rewrite it with $(9.17)$ and kernel:\\[g_\\text{SVM}(\\boldsymbol x)=\\text{sign}\\left(\\sum_{n=1}^N\\alpha_ny_nK(\\boldsymbol x_n,\\boldsymbol x)+b\\right) \\tag{9.24}\\]Polynomial KernelConsider a general $2$nd order polynomial transform as follows\\[\\boldsymbol\\Phi_2(\\boldsymbol x)=(1,x_1,x_2,...,x_d,x_1^2,x_2^2,...,x_d^2) \\tag{9.25}\\]the kernel function is $K_2(\\boldsymbol x,\\boldsymbol x’)=1+\\boldsymbol x^\\text T\\boldsymbol x’+(\\boldsymbol x^\\text T\\boldsymbol x’)(\\boldsymbol x^\\text T\\boldsymbol x’)$, if we set\\[\\boldsymbol\\Phi_2(\\boldsymbol x)=(1,\\sqrt{2\\gamma} x_1,\\sqrt{2\\gamma}x_2,...,\\sqrt{2\\gamma}x_d,\\gamma x_1^2,\\gamma x_2^2,...,\\gamma x_d^2) \\tag{9.26}\\]then we have a simple form of kernel\\[\\begin{split}K_2(\\boldsymbol x,\\boldsymbol x&#39;)&amp;amp;=1+2\\gamma \\boldsymbol x^\\text T\\boldsymbol x&#39;+\\gamma^2(\\boldsymbol x^\\text T\\boldsymbol x&#39;)(\\boldsymbol x^\\text T\\boldsymbol x&#39;) \\\\[1em]&amp;amp;=(1+\\gamma \\boldsymbol x^\\text T\\boldsymbol x&#39;)^2\\end{split}\\tag{9.27}\\]further, we define the General Polynomial Kernel shaped like\\[K_Q(\\boldsymbol x,\\boldsymbol x&#39;)=(\\zeta+\\gamma \\boldsymbol x^\\text T\\boldsymbol x)^Q \\tag{9.28}\\]where $\\zeta \\ge 0$ and $\\gamma &amp;gt; 0$.Gaussian KernelWe can use Gaussian function to implement mapping to infinite-dim space, and the Gaussian Kernel in this process is defined as\\[K(\\boldsymbol x,\\boldsymbol x&#39;)=\\exp\\left(-\\gamma\\Vert\\boldsymbol x-\\boldsymbol x&#39;\\Vert^2\\right) \\tag{9.29}\\]Next we consider different choices of $\\gamma$obviously, we find that a too large $\\gamma$ results in overfit. Therefore, a relatively small $\\gamma$ is recommendable.Mercer’s ConditionSufficient and necessary conditions for valid kernel: Symmetric Let $k_{ij}=K(x_i,x_j)$, then we have\\[\\begin{split}\\\\\\boldsymbol K &amp;amp;= \\left(\\begin{array}{ccc}\\boldsymbol\\Phi^\\text T(x_1)\\boldsymbol\\Phi(x_1) &amp;amp; \\boldsymbol\\Phi^\\text T(x_1)\\boldsymbol\\Phi(x_2) &amp;amp; \\cdots &amp;amp; \\boldsymbol\\Phi^\\text T(x_1)\\boldsymbol\\Phi(x_N) \\\\[1em]\\boldsymbol\\Phi^\\text T(x_2)\\boldsymbol\\Phi(x_1) &amp;amp; \\boldsymbol\\Phi^\\text T(x_2)\\boldsymbol\\Phi(x_2) &amp;amp; \\cdots &amp;amp; \\boldsymbol\\Phi^\\text T(x_2)\\boldsymbol\\Phi(x_N) \\\\[1em]\\vdots &amp;amp; \\vdots &amp;amp; \\ddots &amp;amp; \\vdots \\\\[1em]\\boldsymbol\\Phi^\\text T(x_N)\\boldsymbol\\Phi(x_1) &amp;amp; \\boldsymbol\\Phi^\\text T(x_N)\\boldsymbol\\Phi(x_2) &amp;amp; \\cdots &amp;amp; \\boldsymbol\\Phi^\\text T(x_N)\\boldsymbol\\Phi(x_N)\\end{array}\\right) \\\\[1em]&amp;amp;= (\\boldsymbol z_1,\\boldsymbol z_2,...,\\boldsymbol z_N)^\\text T (\\boldsymbol z_1,\\boldsymbol z_2,...,\\boldsymbol z_N) \\\\[1em]&amp;amp;= \\boldsymbol Z\\boldsymbol Z^\\text T\\end{split}\\tag{9.30}\\] the matrix $\\boldsymbol K$ must always be positive semi-definite. " }, { "title": "Regularization", "url": "/posts/Regularization/", "categories": "Notes, Machine Learning", "tags": "ML, 正则化, 过拟合", "date": "2018-07-20 00:00:00 +0800", "snippet": "Factors Leading to Overfitting Number of samples is too small Too much noise Excessively complicated modelWeight-decay RegularizationFirstly, look at a typical example of overfitting:As shown, a high-order polynomial (such as 10th-order) performed not very well when dataset is not large enough. One of our ideas to reduce overfitting is ‘stepping back’ from $\\mathcal H_{10}$ to $\\mathcal H_2$. For convenience, we denote $\\tilde {\\boldsymbol w}$ as $\\boldsymbol w$. For $x \\in \\mathbb R$, we have\\[\\begin{split}\\mathcal H_{10}&amp;amp;:w_0+w_1x+w_2x^2+w_3x^3\\cdots+w_{10}x \\\\[1em]\\mathcal H_2&amp;amp;:w_0+w_1x+w_2x\\end{split}\\tag{8.1}\\]obviously, $\\mathcal H_{10}=\\mathcal H_2$ when $w_3=w_4=\\cdots=w_{10}=0$. Make this constraint a little looser, we set\\[\\mathcal H_2&#39;=\\{\\ \\boldsymbol w\\in \\mathbb R^{10+1}\\ \\ \\text{while more than } 8 \\text{ of } w_q=0\\} \\tag{8.2}\\]then regression with $\\mathcal H_2’$ is\\[\\begin{split}\\min_{\\boldsymbol w\\in \\mathbb R^{10+1}}&amp;amp; \\ E_{\\text{in}}(\\boldsymbol w) \\\\[1em]\\text{s.t.}&amp;amp; \\ \\sum_{q=0}^{10}[\\![w_q\\ne0]\\!] \\le 3\\end{split}\\tag{8.3}\\]However, bad news is that optimization of this formula is NP-hard to solve, we have to replace it with a softer constraint as follows:\\[\\mathcal H_2&#39;=\\{\\ \\boldsymbol w\\in \\mathbb R^{10+1}\\ \\ \\text{while } \\Vert\\boldsymbol w\\Vert^2 \\le C \\} \\tag{8.4}\\]regression is\\[\\begin{split}\\min_{\\boldsymbol w\\in \\mathbb R^{10+1}}&amp;amp; \\ E_{\\text{in}}(\\boldsymbol w) \\\\[1em]\\text{s.t.}&amp;amp; \\ \\sum_{q=0}^{10}w_q^2 \\le C\\end{split}\\tag{8.5}\\]Similar to linear regression, we record the in-sample error as$E_{\\text{in}}(\\boldsymbol w)=\\frac{1}{N}\\Vert\\boldsymbol {Zw}-\\boldsymbol y\\Vert^2$. Consider the general form when feature dimension is $Q$, we have optimization goal:\\[\\begin{split}\\min_{\\boldsymbol w\\in \\mathbb R^{Q+1}}&amp;amp; \\ \\frac{1}{N}(\\boldsymbol {Zw}-\\boldsymbol y)^{\\text T}(\\boldsymbol {Zw}-\\boldsymbol y) \\\\[1em]\\text{s.t.}&amp;amp; \\ \\boldsymbol w^{\\text T}\\boldsymbol w \\le C\\end{split}\\tag{8.6}\\]we call this model Regularized Hypothesis, and the optimal weight vector of which is recorded as $\\boldsymbol w_{\\text{REG}}$.Next we try to solve this optimization problem. Look at the image belowin non-constraint optimization, we search in the opposite direction of the gradient to find the optimal solution. However, feasible $\\boldsymbol w$ here must be within a radius-$\\sqrt C$ hypersphere and in most cases the optimal solution will be at the edge of it. Suppose there already have a $\\boldsymbol w$ at the edge, then we can judge whether it is the optimal by judging whether it can proceed in the opposite direction of gradient under constraint. And we can see from the figure clearly that only if $\\boldsymbol w$ walk along the tangential direction of the hypersphere can it stays within the hypersphere. Focus on the green vector in above figure, if -$\\nabla E_{\\text{in}}(\\boldsymbol w)$ has a component in the tangential direction of the hypersphere, we can get a better $\\boldsymbol w$ by the direction of this component. That is, the $\\boldsymbol w$ is optimal only when $\\nabla E_{\\text{in}}(\\boldsymbol w)$ is in the same direction with $\\boldsymbol w$, the normal vector of the hypersphere. So we try to find Lagrange multiplier $\\lambda&amp;gt;0$ and $\\boldsymbol w_\\text{REG}$ such that:\\[\\nabla E_{\\text{in}}(\\boldsymbol w_\\text{REG})+\\frac{2\\lambda}{N}\\boldsymbol w_\\text{REG}=\\boldsymbol 0 \\tag{8.7}\\]where $\\frac{2\\lambda}{N}$ is set in order to facilitate subsequent calculation. Similar to linear regression, we rewrite $(8.7)$ as follows:\\[\\begin{split}&amp;amp;\\frac{2}{N}(\\boldsymbol {Z}^\\text T\\boldsymbol {Z}\\boldsymbol w_\\text{REG}-\\boldsymbol {Z}^\\text T\\boldsymbol y)+\\frac{2\\lambda}{N}\\boldsymbol w_\\text{REG}&amp;amp;=\\boldsymbol 0 \\\\[1em]&amp;amp;\\Rightarrow (\\boldsymbol {Z}^\\text T\\boldsymbol {Z}\\boldsymbol w_\\text{REG}-\\boldsymbol {Z}^\\text T\\boldsymbol y)+\\lambda\\boldsymbol w_\\text{REG}&amp;amp;=\\boldsymbol 0\\end{split}\\tag{8.8}\\]Solve this equation, we have\\[\\boldsymbol w_\\text{REG}=(\\boldsymbol {Z}^\\text T\\boldsymbol {Z}+\\lambda\\boldsymbol I)^{-1}\\boldsymbol {Z}^\\text T\\boldsymbol y \\tag{8.9}\\]which we called Ridge Regresson in statistics. It’s easy to prove $\\boldsymbol {Z}^\\text T\\boldsymbol {Z}+\\lambda\\boldsymbol I$ is positive-definite when $\\lambda&amp;gt;0$.Regard $(8.7)$ as the derivative of $E_\\text{in}(\\boldsymbol w)+\\frac{\\lambda}{N}\\boldsymbol w^\\text T\\boldsymbol w$, which is called Augmented Error, recorded as $E_\\text{aug}(\\boldsymbol w)$, and where $\\boldsymbol w^\\text T\\boldsymbol w$ is called Regularizer. This kind of regularization preferring a shorter $\\boldsymbol w$ is called Weight-decay Regularization. Lagrange multiplier $\\lambda$ is related to $C$, represents the strength of constraint, in this opinion, we think that $\\lambda=0$ is also acceptable because it represents a regression without restraint. We can get different results using different $\\lambda$ as follows:By the way, if we limit each component of $\\boldsymbol x$ within scope of $[-1,1]$, the $n$th-oreder polynomial would be so little that penalty $\\lambda$ seems too heavy. In this case, we can replace simple polynomials such as $x,x^2,x^3,…x^n$ with Legendre Polynomials. It may have a better fitting result.Connection to VC TheoryLook at this problem from the perspective of VC theory, we have\\[\\begin{gather}E_\\text{aug}(\\boldsymbol w) = &amp;amp;E_\\text{in}(\\boldsymbol w)+\\frac{\\lambda}{N}\\boldsymbol w^\\text T\\boldsymbol w \\tag{8.10} \\\\[1em]E_\\text{out}(\\boldsymbol w) \\le &amp;amp;E_\\text{in}(\\boldsymbol w)+\\Omega(\\mathcal H) \\tag{8.11}\\end{gather}\\]where $\\boldsymbol w^\\text T\\boldsymbol w$ represents complexity of a single hypothesis and $\\Omega(\\mathcal H)$ represents complexity of whole hypothesis set. When constraint $C$ introduced, effective hypotheses have been grately reduced. Record effective VC dimension of hypothesis set as $d_\\text{EFF}(\\mathcal {H,A} )$, where $\\mathcal A$ represents regularized algorithm under constraint. When $\\lambda&amp;gt;0$, we have:\\[d_\\text{EFF}(\\mathcal{H,A}) \\le d_\\text{VC} \\tag{8.12}\\]As $\\lambda$ increased, more $\\boldsymbol w$ will be discarded and $d_\\text{EFF}$ will be lower." }, { "title": "Nonlinear Transformation", "url": "/posts/Nonlinear-Transformation/", "categories": "Notes, Machine Learning", "tags": "ML, 分类", "date": "2018-07-13 00:00:00 +0800", "snippet": "Consider classification problem as follows:obviously, it is a nonlinear separable problem, but we can use a circle to reach our goal:hypothesis of this model can be written as\\[h(\\boldsymbol x)=\\text{sign}(0.6-x_1^2-x_2^2) \\tag{7.1}\\]next we set $\\boldsymbol z = (1,x_1^2,x_2^2)^{\\text T}$ and $\\tilde{\\boldsymbol w}=(0.6,-1,-1)^{\\text T}$, then rewrite $h$\\[h(\\boldsymbol z)=\\tilde{\\boldsymbol w}^{\\text T}\\boldsymbol z \\tag{7.2}\\]In this transform, we map the point in space of $\\boldsymbol x$ to space of $\\boldsymbol z$, and these point is linear separable in the latter, we call this process Feature Transformation.\\[\\boldsymbol x \\in \\mathcal X \\mathop\\longmapsto^{\\boldsymbol\\Phi} \\boldsymbol z \\in \\mathcal Z \\tag{7.3}\\]In the problem above, $\\boldsymbol z=\\boldsymbol\\Phi(\\boldsymbol x)=(1,x_1^2,x_2^2)^{\\text T}$.Further, we have a general form of map function in quadratic hypotheses:\\[\\boldsymbol\\Phi_2(\\boldsymbol x)=(1,x_1,x_2,x_1^2,x_1x_2,x_2^2)^{\\text T} \\tag{7.4}\\]From this formula we find that $\\boldsymbol\\Phi_1(\\boldsymbol x)=(1,x_1,x_2)^{\\text T}$ is a degenerate version of $\\boldsymbol\\Phi_2(\\boldsymbol x)$, that means all components in $\\boldsymbol\\Phi_1$ is also included in $\\boldsymbol\\Phi_2$ and $\\boldsymbol\\Phi_2$ just adds the quadratic combination of $x_i$.Then consider the general situation when sample vector $\\boldsymbol x \\in \\mathbb R^{d+1}$ and feature dimension is $Q$, then we have\\[\\boldsymbol\\Phi_Q(\\boldsymbol x) = (\\boldsymbol\\Phi_{Q-1}(x),x_1^Q,x_1^{Q-1}x_2,\\cdots,x_d^Q) \\tag{7.5}\\]With feature transformation, we can use linear model to solve more problems that were previously difficult to slove. But it is important to note that when we do $Q$-th order polynomial transform where $Q$ is quite large, $\\boldsymbol\\Phi_Q(\\boldsymbol x)$ here may be difficult to compute or store." }, { "title": "Linear Model for Classification", "url": "/posts/Linear-Model-for-Classification/", "categories": "Notes, Machine Learning", "tags": "ML, 线性模型, 分类, 算法", "date": "2018-07-13 00:00:00 +0800", "snippet": "Summary of Error FunctionAll three algorithm we mentioned above have the same linear scoring function $s=\\boldsymbol w^{\\text T}\\boldsymbol x$, now we discuss about the similarity of these algorithm in binary classification $\\mathcal Y\\in \\lbrace-1,+1\\rbrace$. Perceptron\\[\\begin{split}h(\\boldsymbol x)&amp;amp;=\\text{sign}(s) \\\\[1em]\\text{err}_{0/1}(s,y)&amp;amp;=[\\![\\text{sign}(s) \\ne y]\\!] \\\\[1em]&amp;amp;=[\\![\\text{sign}(ys) \\ne +1]\\!]\\end{split}\\] Linear rergreesion\\[\\begin{split}h(\\boldsymbol x)&amp;amp;=s \\\\[1em]\\text{err}_{\\text{SQR}}(s,y)&amp;amp;=(s-y)^2 \\\\[1em]&amp;amp;=(ys-1)^2\\end{split}\\] Logistic regression\\[\\begin{split}h(\\boldsymbol x)&amp;amp;=\\sigma(s) \\\\[1em]\\text{err}_{\\text{CE}}(s,y)&amp;amp;=\\ln(1+\\exp(-ys)) \\\\[1em]\\text{scaled:} \\ \\ \\text{err}_{\\text{SCE}}(s,y)&amp;amp;=\\log_2(1+\\exp(-ys))\\end{split}\\] Draw these error function on an image:as shown, $\\rm err_{SCE}$ is an upper bound of $\\rm err_{0/1}$, therefore if we can limit $\\rm err_{SCE}$ to a small value, then $\\rm err_{0/1}$ should also be small. And VC bound theory gives a more powerful proof of it. Thus we can safely use linear models for classification.Finally, we compare the pros and cons of these algorithm in classification: Perceptron Pros: $\\text{err}_{0/1}$ can be reduced to the lowest value if dataset is linear separable. Cons: deteriorated to a NP-hard problem if dataset is non-linear separable. Linear regression Pros: easy to do optimize Cons: it cannot ensure $\\text{err}_{0/1}$ is small enough when $ys$ is very small or very large. Logistic regression Pros: comparably easy to optimize Cons: it cannot ensure $\\text{err}_{0/1}$ is small enough when $ys$ is very small. Stochastic Gradient DescentWe have known both PLA and logistic regression realize optimization through iteration, that isFor $t=0,1,…$ do\\[\\boldsymbol w_{t+1} \\gets \\boldsymbol w_t+\\eta\\boldsymbol v \\tag{6.1}\\]When stop, return $\\boldsymbol w$ as $g$.Difference is that each iteration of PLA correct only one point, while logistic regression calculates the contribution to gradient of every point and take the average, so each iteration has a complexity of $O(n)$.To reduce the complexity, we select a point randomly and calculates its contribution to gradient $\\nabla_{\\boldsymbol w}\\text{err}(\\boldsymbol w, \\boldsymbol x_n, y_n)$, called Stochastic Gradient. And we can regard the real gradient as the expectation of stochastic gradient. From another side, we also regard the stochastic gradient as sum of real gradient and a zero-mean noise. In this way, we replace real gradient with stochastic gradient, called Stochastic Gradient Descent. After enough iteration, we can also get a acceptable result. Each iterative formula as follows:\\[\\boldsymbol w_{t+1} \\gets \\boldsymbol w_t+\\eta \\cdot \\boldsymbol \\sigma(-y_n\\boldsymbol w^{\\text T}\\boldsymbol x)(y_n\\boldsymbol x_n) \\tag{6.2}\\]Look at the iterative formula of PLA again:\\[\\boldsymbol w_{t+1} \\gets \\boldsymbol w_t+1\\cdot [\\![y_n \\ne \\text{sign}(\\boldsymbol w^\\text{T}\\boldsymbol x)]\\!] y_n\\boldsymbol x_n \\tag{6.3}\\]where $y_n \\ne \\text{sign}(\\boldsymbol w^\\text{T}\\boldsymbol x)$ means PLA update $\\boldsymbol w$ only when an error point $\\boldsymbol x_n$ founded. From the two formula above, SGD logistic regression can be regard as a ‘soft’ PLA, because in PLA we take whether $y_n \\ne \\text{sign}(\\boldsymbol w^\\text{T}\\boldsymbol x)$ as aJudging criteria, while in SGD logistic regression, we only consider how close they are. By the way, if we ensure $\\boldsymbol w^{\\text T}\\boldsymbol x$ is big enough, then $\\eta=1$ may be a good choice.When using SGD logistic regression, we usually stop iteration when $t$ is large enough and use $\\eta \\approx 0.1$ when $\\boldsymbol x$ in proper range.Muticlass ClassificationOne-Versus-AllAssuming that there are four categories in the question, each time we treat one of the categories as positive , while all other categories become negative, it can build a binary classifier. Keeping loop, and finally we will get 4 classifiers as following figure:But when all the four classifiers put a sample into negative class, such as the middle area of the figure above. what should we do?It’s easy if we use logistic regression and set each classifier output a probality of a sample belonging to its own category. We can simply choose the highest probability of each classifier as the final result.\\[\\begin{split}g(\\boldsymbol x) &amp;amp;= \\mathop{\\arg\\min}_{k\\in \\mathcal Y} \\ \\sigma(\\boldsymbol w^{\\text T}_{[k]}\\boldsymbol x) \\\\[1em]&amp;amp;=\\mathop{\\arg\\min}_{k\\in \\mathcal Y} \\ \\boldsymbol w^{\\text T}_{[k]}\\boldsymbol x \\ \\ \\ \\ (\\sigma\\text{ is monotonic})\\end{split}\\tag{6.4}\\]We call this method One-Versus-All (OVA) decomposition.One-Versus-OneIt is important to note OVA can’t perform very well when $\\vert\\mathcal Y\\vert$ is large. Considering this situation when $\\vert\\mathcal Y\\vert=100$ and number of samples in each category is not much different. If we choose a classifier which put all samples into negative class, then its accuracy rate has reached 99%. Therefore in OVA we might get 100 classifiers each of which judge all samples as negative.We can use One-Versus-One (OVO) decomposition to solve the problem. OVO chooses only two of all categories to bulid a classifier as follows:after bulid all classifiers (whose number should be $\\binom {\\vert\\mathcal Y\\vert}{2}$), we use ‘voting method’ to get the final result." }, { "title": "Logistic Regression", "url": "/posts/Logistic-Regression/", "categories": "Notes, Machine Learning", "tags": "ML, 逻辑回归, 算法", "date": "2018-07-12 00:00:00 +0800", "snippet": "In some cases, we want to learn the probability of something happening. In order to meet this goal, we can use a sigmoid function $\\sigma$ which map the result of linear regression to a range between $0-1$, One of the most commonly used $\\sigma$ is Logistic Function:\\[\\sigma(x)=\\frac{1}{1+\\text e^{-x}} \\tag{5.1}\\]then we have hypotheses of the form:\\[h(\\boldsymbol x) = \\frac{1}{1+\\text e^{-\\boldsymbol w^{\\text T}\\boldsymbol x}} \\tag{5.2}\\]Next we try to measure the error function of the hypotheses. Consider the target function representing the probability of $y=+1$, that is $f(\\boldsymbol x)=P(+1\\vert\\boldsymbol x)$, we have\\[P(y|\\boldsymbol x)=\\begin{cases}f(\\boldsymbol x) &amp;amp; y=+1 \\\\ \\\\1-f(\\boldsymbol x) &amp;amp; y=-1\\end{cases}\\tag{5.3}\\]Consider a dataset $\\mathcal D=\\lbrace(\\boldsymbol x_1,+1),(\\boldsymbol x_2,-1),…,(\\boldsymbol x_N,-1)\\rbrace$, we can calculate the probality that $f$ generates $\\mathcal D$\\[\\begin{split}P(f) &amp;amp;= P(\\boldsymbol x_1)P(+1|\\boldsymbol x_1) \\times P(\\boldsymbol x_2)P(-1|\\boldsymbol x_2) \\times \\cdots \\times P(\\boldsymbol x_N)P(-1|\\boldsymbol x_N) \\\\[1em]&amp;amp;=P(\\boldsymbol x_1)f(\\boldsymbol x_1) \\times P(\\boldsymbol x_2)(1-f(\\boldsymbol x_2)) \\times \\cdots \\times P(\\boldsymbol x_N)(1-f(\\boldsymbol x_N))\\end{split}\\tag{5.4}\\]Similarly, for a single hypothesis $h$, we have the probability that $h$ generates $\\mathcal D$ called Likelihood\\[\\text{likelihood}(h)=P(\\boldsymbol x_1)h(\\boldsymbol x_1) \\times P(\\boldsymbol x_2)(1-h(\\boldsymbol x_2)) \\times \\cdots \\times P(\\boldsymbol x_N)(1-h(\\boldsymbol x_N)) \\tag{5.5}\\]Then if $h \\approx f$, it should have $\\text{likelihood}(h) \\approx P(f)$. Actually we know the $f$ as our target function, so the probability using $f$ generates $\\mathcal D$ is usually large. So our goal is to maximum the likelihood of $h$, and the final hypothesis $g$ can be founded\\[g=\\mathop{\\arg\\max}_h \\ \\text{likelihood}(h) \\tag{5.6}\\]In addition, for logistic function we have a property as follows:\\[1-\\sigma(x)=\\sigma(-x) \\tag{5.7}\\]according to this property, we can rewrite $(5.5)$\\[\\begin{split}\\text{likelihood}(h)&amp;amp;=P(\\boldsymbol x_1)h(+\\boldsymbol x_1) \\times P(\\boldsymbol x_2)h(-\\boldsymbol x_2) \\times \\cdots \\times P(\\boldsymbol x_N)h(-\\boldsymbol x_N) \\\\[1em]&amp;amp;=\\prod_{n=1}^NP(\\boldsymbol x_n)h(y_n\\boldsymbol x_n)\\end{split}\\tag{5.8}\\]another hand, $P(\\boldsymbol x_n)$ is already known, so we have\\[\\text{likelihood}(h)\\propto \\prod_{n=1}^Nh(y_n\\boldsymbol x_n) \\tag{5.9}\\]Replace $h$ with $\\boldsymbol w$ as argument and replace products with summation to avoid overflow, we can do following modification:\\[\\begin{split}\\max_h\\prod_{n=1}^Nh(y_n\\boldsymbol x_n) &amp;amp;= \\max_{\\boldsymbol w} \\sum_{n=1}^N\\ln\\sigma(y_n\\boldsymbol w^{\\text T}\\boldsymbol x_n) \\\\[1em]&amp;amp;= \\min_{\\boldsymbol w} \\sum_{n=1}^N-\\ln\\sigma(y_n\\boldsymbol w^{\\text T}\\boldsymbol x_n) \\\\[1em]&amp;amp;= \\min_{\\boldsymbol w} \\sum_{n=1}^N\\ln(1+\\text e^{-y_n\\boldsymbol w^{\\text T}\\boldsymbol x_n})\\end{split}\\tag{5.10}\\]Record $\\frac{1}{N}\\sum_{n=1}^N\\ln(1+\\text e^{-y_n\\boldsymbol w^{\\text T}\\boldsymbol x_n})$ as $E(\\boldsymbol w)$ which represents the mean error function of the whole dataset. Fortunately, it is continuous, differentiable, twice-differentiable and convex. Similar to before, all we need to do is making the gradient $\\boldsymbol 0$.According to the chain-rule, we have partial derivation of $E(\\boldsymbol w)$:\\[\\begin{split}\\frac{\\partial E(\\boldsymbol w)}{\\partial \\boldsymbol w} &amp;amp;=\\frac{1}{N}\\sum_{n=1}^N\\frac{\\partial \\ln(1+\\text e^{-y_n\\boldsymbol w^{\\text T}\\boldsymbol x_n})}{\\partial (1+\\text e^{-y_n\\boldsymbol w^{\\text T}\\boldsymbol x_n})} \\ \\cdot \\ \\frac{\\partial (1+\\text e^{-y_n\\boldsymbol w^{\\text T}\\boldsymbol x_n})}{\\partial{-y_n\\boldsymbol w^{\\text T}\\boldsymbol x_n}} \\ \\cdot \\ \\frac{\\partial{-y_n\\boldsymbol w^{\\text T}\\boldsymbol x_n}}{\\partial \\boldsymbol w} \\\\[1em]&amp;amp;=\\frac{1}{N} \\sum_{n=1}^N\\frac{\\text e^{-y_n\\boldsymbol w^{\\text T}\\boldsymbol x_n}}{1+\\text e^{-y_n\\boldsymbol w^{\\text T}\\boldsymbol x_n}} \\ \\cdot \\ -y_n\\boldsymbol x_n \\\\[1em]&amp;amp;=\\frac{1}{N} \\sum_{n=1}^N\\sigma(-y_n\\boldsymbol w^{\\text T}\\boldsymbol x_n)(-y_n\\boldsymbol x_n)\\end{split}\\tag{5.11}\\]Obviously, equation $\\nabla_{\\boldsymbol w}E=0$ has no closed-form solution. For such problems, we usually solve it using iterative optimization. In each iteration, we update $\\boldsymbol w_{t+1}$ with $\\boldsymbol w_t+\\eta\\boldsymbol v$, where $\\eta$ is called learning rate and $\\boldsymbol v$ is a unit vector representing the direction of each update.For logistic regression, we use an optimization method called Gradient Descent to find the optimal solution.According Taylor expansion, in a neighbourhood of $\\boldsymbol w$ we have\\[E(\\boldsymbol w+\\eta\\boldsymbol v) \\approx E(\\boldsymbol w)+\\eta\\boldsymbol v^{\\text T}\\nabla E(\\boldsymbol w) \\tag{5.12}\\]Based on the above derivation, our optimization goal is $E(\\boldsymbol w)$. In each iteration, it can rewrite as follows:\\[\\min E(\\boldsymbol w+\\eta\\boldsymbol v)=\\min \\ (E(\\boldsymbol w)+\\eta\\boldsymbol v^{\\text T}\\nabla E(\\boldsymbol w)) \\tag{5.13}\\]$\\boldsymbol w$ and $\\nabla E(\\boldsymbol w)$ in current iteration is known, $\\eta$ is a given positive number and $\\Vert\\boldsymbol v\\Vert=1$, the only thing we need to determine is the direction of $\\boldsymbol v$. So our goal can be simplify as\\[\\min_{\\boldsymbol v} \\ \\boldsymbol v^{\\text T}\\nabla E(\\boldsymbol w) \\tag{5.14}\\]Obviously, only if $\\boldsymbol v$ and $\\nabla E(\\boldsymbol w)$ have the opposite directions can this formula get the minimum value. So we have\\[\\boldsymbol v=-\\frac{\\nabla E(\\boldsymbol w)}{\\Vert\\nabla E(\\boldsymbol w)\\Vert} \\tag{5.15}\\]and the iterative formula of $\\boldsymbol w$ is $\\boldsymbol w_{t+1} \\gets \\boldsymbol w_t-\\eta\\frac{\\nabla E(\\boldsymbol w)}{\\Vert\\nabla E(\\boldsymbol w)\\Vert}$.About the choice of learing rate $\\eta$, both too large or too small are not a good choice. $\\eta$ is better to be monotonic of $\\Vert\\nabla E(\\boldsymbol w)\\Vert$. Set $\\eta \\gets \\frac{\\eta}{\\Vert\\nabla E(\\boldsymbol w)\\Vert}$, we have a new iterative formula $\\boldsymbol w_{t+1} \\gets \\boldsymbol w_t-\\eta \\nabla E(\\boldsymbol w)$." }, { "title": "Linear Regression", "url": "/posts/Linear-Regression/", "categories": "Notes, Machine Learning", "tags": "ML, 线性回归, 算法", "date": "2018-07-12 00:00:00 +0800", "snippet": "Given a dataset $\\mathcal D = \\lbrace(\\boldsymbol {x_i},y_i)\\rbrace_{i=1}^N$, where $\\boldsymbol x \\in \\mathbb R^d$. we try to get a weight vector $(w_1,w_2,…,w_d)^\\text T$ such that\\[y_i \\approx \\sum_{i=1}^d w_ix_i + b \\tag{4.1}\\]Set $\\boldsymbol w =(b,w_1,…,w_d)^\\text T$, $\\boldsymbol x=(1,x_1,…,x_d)^\\text T$. Then the linear combination of $\\boldsymbol w$ and $\\boldsymbol x$ is a hypothesis recorded as $h(\\boldsymbol x)$.Based on least square method, we have squared error $E(\\hat y,y)=(\\hat y-y)^2$. And our goal is to minimize it. Next we do some calculations as follows:\\[\\begin{split}E(\\boldsymbol w) &amp;amp;= \\frac{1}{N}\\sum_{n=1}^N \\left(\\boldsymbol w^\\text T \\boldsymbol x_n - y_n\\right)^2 \\\\ \\\\&amp;amp;=\\frac{1}{N}\\left|\\left|\\begin{array}{ccc}\\boldsymbol x_1^{\\text T} \\boldsymbol w - y_1 \\\\\\boldsymbol x_2^{\\text T} \\boldsymbol w - y_2 \\\\\\vdots \\\\\\boldsymbol x_N^{\\text T} \\boldsymbol w - y_N \\\\\\end{array}\\right|\\right|^2 \\\\ \\\\&amp;amp;=\\frac{1}{N}||\\boldsymbol {Xw-y}||^2\\end{split}\\tag{4.2}\\]where $\\boldsymbol X=(\\boldsymbol x_1^\\text{T};\\boldsymbol x_2^\\text{T};…;\\boldsymbol x_N^\\text{T})$, $\\boldsymbol y=(y_1,y_2,…,y_N)^\\text T$.Then our goal is\\[\\min_{\\boldsymbol w}\\frac{1}{N}||\\boldsymbol {Xw-y}||^2 \\tag{4.3}\\]The target function $E(\\boldsymbol w)$ is continuous, differentiable and convex. So we have the neccessary condition of optimal $\\boldsymbol w$ :\\[\\nabla E(\\boldsymbol w)=\\boldsymbol 0 \\tag{4.4}\\]After derivation, we have\\[\\frac{2}{N}(\\boldsymbol {X}^\\text T\\boldsymbol {Xw}-\\boldsymbol {X}^\\text T\\boldsymbol y)=\\boldsymbol 0 \\tag{4.5}\\]If matrix $\\boldsymbol {X}^\\text T\\boldsymbol {X}$ is invertible, it’s easy to get the solution of $(4.5)$:\\[\\boldsymbol w_{\\text{lin}}=(\\boldsymbol {X}^\\text T\\boldsymbol {X})^{-1}\\boldsymbol {X}^\\text T\\boldsymbol y \\tag{4.6}\\]Even if $\\boldsymbol {X}^\\text T\\boldsymbol {X}$ is a singular matrix, No need to worry too much because most of programs computing inverse matrix can deal with it easily." }, { "title": "Perceptron", "url": "/posts/Perceptron/", "categories": "Notes, Machine Learning", "tags": "ML, 感知机, 算法", "date": "2018-07-11 00:00:00 +0800", "snippet": "For $\\boldsymbol{x}=(x_1,x_2,…,x_d)$, compute a weighted ‘score’. Judging as positive if $\\sum_{i=1}^dw_ix_i&amp;gt;\\text{threshold}$ while judging as negative if $\\sum_{i=1}^dw_ix_i&amp;lt;\\text{threshold}$.Using $\\mathcal{Y}:\\lbrace+1,-1\\rbrace$ represent two classification results, linear formula $h\\in\\mathcal{H}$ are\\[h(\\boldsymbol{x})=\\text{sign}\\left(\\left(\\sum_{i=1}^dw_ix_i\\right)-\\text{threshold}\\right) \\tag{3.1}\\]In $(3.1)$, set $-\\text{threshold}=w_0$ and $+1=x_0$. Then\\[\\begin{split}h(\\boldsymbol{x})&amp;amp;=\\text{sign}\\left(\\sum_{i=0}^dw_ix_i\\right) \\\\&amp;amp;=\\text{sign}\\left(\\boldsymbol{w}^\\text{T}\\boldsymbol{x}\\right)\\end{split}\\tag{3.2}\\]In order to find a best straight line to completely separate all the positive and negative examples on the plane, we can use the idea of ‘point-by-point correction’. First, take a straight line on the plane to see which points are classified incorrectly. Then start to correct the error point in the first attempt, that is, change the position of the line, so that the error point becomes classified correctly. Then, the second, third, and so on until all the error classification points are separated correctly in a straight line.Next we introduce the perceptron learning algorithm (PLA). First, randomly select a line to classify. Then find the first point of the classification error. If this point represents a positive example but misclassified as a negative example, that is $\\boldsymbol w_t^\\mathrm{T} \\boldsymbol x_{n(t)} &amp;lt; 0$, which means the angle between $\\boldsymbol w_t$ and $\\boldsymbol x_{n(t)}$ is over $\\frac{\\pi}{2}$, and where $\\boldsymbol w_t$ is the normal vector of the line. Therefore, $\\boldsymbol x_{n(t)}$ is mis-divided on the lower side of the line (relative to the normal vector, the direction of the normal vector is the side where the positive class is located). The correction method is to make $\\boldsymbol w_{t}$ and $\\boldsymbol x_{n(t)}$ less than $\\frac{\\pi}{2}$. A common way to do this is set $\\boldsymbol w_{t+1} \\gets \\boldsymbol w_t + \\boldsymbol x_{n(t)}$.Similarly, if a point is misclassified as negative example, that means $\\boldsymbol w_t$ and $\\boldsymbol x_{n(t)}$are less than $\\frac{\\pi}{2}$, where $\\boldsymbol w_t$ is the normal vector of the line. So, $\\boldsymbol x_{n(t)}$ is misclassified in the line on the upper side, the correction is to make $\\boldsymbol w_t$ and $\\boldsymbol x_{n(t)}$ greater than $\\frac{\\pi}{2}$ with $\\boldsymbol w_{t+1} \\gets \\boldsymbol w_t-\\boldsymbol x_{n(t)}$.In summary, when an error point found, we have iterative formula of PLA\\[\\boldsymbol w_{t+1} \\gets \\boldsymbol w_t+y_{n(t)}\\boldsymbol x_{n(t)} \\tag{3.3}\\]PLA works well in 3D or even higher space, as shown in the figure above, the sample point is located on the lower side of plane determined by $\\boldsymbol w$ but it located on the upper side of plane after modification.According to the method mentioned above, PLA keeps finding error point and iterating. It is important to note that in each iteration it may cause the previously classified point to become a wrong point. But it does’t matter because constant iteration will eventually classify all points correctly (the premise is linear separability).In practice, we can check each point one by one, and correct the wrong point immediately. Keep iteration until all points are classified correctly. At this time, complexity of each iteration is $O(1)$." }, { "title": "Computational Theory", "url": "/posts/Computational-Theory/", "categories": "Notes, Machine Learning", "tags": "ML, 计算理论", "date": "2018-07-11 00:00:00 +0800", "snippet": "Components of Machine Learning Input (sample vector): $\\boldsymbol{x}\\in\\mathcal{X}$ Output: $y\\in\\mathcal{Y}$ Unknown pattern to be leared (target function): $f:\\mathcal{X}\\to\\mathcal{Y}$ Data set (training examples): $\\mathcal{D}=\\lbrace(\\boldsymbol{x_i},y_i)\\rbrace_{i=1}^N$ Hypotheses set (set of candidate formula): $\\mathcal{H}=\\lbrace h\\vert h:\\mathcal{X}\\to \\mathcal{Y}\\rbrace$ Learning algorithm: $\\mathcal A$ Final hypothesis (learned formula with hopefully good performance): $g:\\mathcal{X}\\to\\mathcal{Y}$Computational TheoryGrowth FunctionRecord the prediction error rate of final hypothesis $g$ in the sample set and the whole input space as $E_{\\text{in}}(g)$ and $E_{\\text{out}}(g)$ respectively. Then we have two major problem of machine learning: $E_{\\text{in}}(g) \\approx E_{\\text{out}}(g)$ $E_{\\text{in}}(g)$ is small enoughWhen each $h\\in\\mathcal H$ is independently distributed, from Hoeffding inequality we have\\[P[\\![|E_{\\text{in}}(g) - E_{\\text{out}}(g) |&amp;gt;\\epsilon]\\!]\\le2M\\cdot\\exp(-2\\epsilon^2N) \\tag{2.1}\\]where $M=\\vert\\mathcal H\\vert,N=\\vert\\mathcal D\\vert$.When number of hypotheses in $\\mathcal H$ is infinite, it have been found that many hypotheses have the same classification results on a specific dataset. We can use this as a standard to divide the hypotheses into finite classes and the number of these effective classes is called Growth Function, recorded as $m_{\\mathcal H}(N)$.\\[m_{\\mathcal H}(N)=\\max_{\\{\\boldsymbol x_1,\\boldsymbol x_2,...,\\boldsymbol x_N\\} \\subseteq \\mathcal X} |\\{h(\\boldsymbol x_1),h(\\boldsymbol x_2),...,h(\\boldsymbol x_N)|h \\in \\mathcal H\\}| \\tag{2.2}\\]It’s upper bound is $2^N$, where $N$ is number of samples.For binary classification problem, every classification result of hypothese in $\\mathcal H$ is called Dichotomy.Here are four kinds of growth function: Positive rays: $m_{\\mathcal H}(N)=N+1$ Positive intervals: $m_{\\mathcal H}(N)=\\frac{1}{2}N^2+\\frac{1}{2}N+1$ Convex sets: $m_{\\mathcal H}(N)=2^N$ 2D perceptrons: $m_{\\mathcal H}(N)&amp;lt;2^N$ in some casesAfter finding the growth function, it’s possible to replace $M$ with $m_{\\mathcal H}(N)$. Obviously, $m_{\\mathcal H}(N) \\ll 2^N$ is what we want to see.If we can find a dataset and all dichotomies of which can be covered by hypotheses set, wa call it Shattered, and it’s growth function $m_{\\mathcal H}(N) = 2^N$.The minimum value of $k$ satisfies $m_{\\mathcal H}(k) &amp;lt; 2^k$ is called a Break Point for $\\mathcal H$.Consider $m_{\\mathcal H}(N)$ as a shotgun, and in each level (level $N$) you can have $m_{\\mathcal H}(N)$ bullets (one bullet for each dichotomy), and you are facing $2^N$ enemies. You have to shoot out and ‘shatter’ off everyone. For the shotgun $m_{\\mathcal H}(N)=2N$, the first and second level you perform well, but in third level, 6 bullets cannot ‘shatter’ off 8 people, so it ‘breaks’.Break point of the four growth function we mentioned before: Positive rays: break point at 2 Positive intervals: break point at 3 Convex sets: no break point 2D perceptrons: break point at 4Bounding FunctionFor 2D perceptrons, we don’t know the exact value of $m_{\\mathcal H}(N)$, but we can find its upper bound $B(N,k)$ called Bounding Function using break point $k$. Fortunately, we finally find a inequality as follows:\\[B(N,k) \\le \\sum_{i=0}^{k-1}\\binom{N}{i} \\tag{2.3}\\]According to this inequlity, we can restrict the growth function under a scope of the polynomial.For 2D perceptrons whose break point is 4, we have\\[m_{\\mathcal H}(N) \\le B(N,k)= \\sum_{I=0}^{4-1} \\binom{N}{i} \\le \\frac{1}{6}N^3+\\frac{5}{6}N+1 \\tag{2.4}\\]It has been proved by mathematical methods that we can replace $M$ with $m_{\\mathcal H}(N)$ in $(2.1)$ and get a new inequality called VC Bound as follows:\\[P[\\![|E_{\\text{in}}(g) - E_{\\text{out}}(g) |&amp;gt;\\epsilon]\\!] \\le 4m_{\\mathcal H}(2N)\\exp(-\\frac{1}{8}\\epsilon^2N) \\tag{2.5}\\]This inequality guarantees the feasibility of mechine learning under a infinite hypotheses space.VC DimensionGenerally, a hypotheses set $\\mathcal H$ is more complicated and powerful when its break point is large. We can use VC Dimension to describe the complexity of $\\mathcal H$, which is defined as the maximum number of points $\\mathcal H$ can ‘shatterd’ off, recorded as $d_{VC}(\\mathcal H)$. If $\\mathcal H$ can ‘shatter’ off no matter how many points, $d_{VC}(\\mathcal H)=\\infty$. It’s easy to find the connection of $d_{VC}$ and $k$ is $d_{VC}+1=k$. Using $d_{VC}$ to describe the upper bound of growth function as follows:\\[B(N,k) \\le \\sum_{i=0}^{d_{VC}}\\binom{N}{i} \\tag{2.6}\\]We can further get the following inequality:\\[B(N,k) \\le \\sum_{i=0}^{d_{VC}}\\binom{N}{i} \\le N^{d_{VC}} \\tag{2.7}\\]After mathematical derivation on $(2.5)$, we further have\\[\\begin{split}P[\\![|E_{\\text{in}}(g) - E_{\\text{out}}(g) |&amp;gt;\\epsilon]\\!] &amp;amp;\\le 4m_{\\mathcal H}(2N)\\exp(-\\frac{1}{8}\\epsilon^2N) \\\\[1em]&amp;amp;\\le 4(2N)^{d_{VC}}\\exp(-\\frac{1}{8}\\epsilon^2N)\\end{split}\\tag{2.8}\\]Thus, the inequality is only related to $k$ and $N$. In general, the sample $N$ is large enough, so usually we just consider the value of $k$. We have following conclusion: If $d_{vc}$ is finite (break point of $\\mathcal H$ exists) and $N$ is largr enough, VC bound ensures the algorithm has good generalization ability. Choose a $g$ from $\\mathcal H$ such that $E_{\\text{in}}(g) \\approx 0$, then its error rate in the whole data will be lower too.In addition, record $4(2N)^{d_{VC}}\\exp(-\\frac{1}{8}\\epsilon^2N)$ as $ \\delta$ and rewrite the inequality $(2.8)$, we have\\[E_{\\text{out}}(g) \\le E_{\\text{in}}(g) + \\underbrace{\\sqrt{\\frac{8}{N} \\ln{\\left(\\frac{4(2N)d_{VC}}{\\delta}\\right)}}}_{\\Omega(N,\\mathcal H,\\delta)} \\tag{2.9}\\]$\\Omega(N,\\mathcal H,\\delta)$ in formula above represents penalty for model complexity, and from which we can see powerful $h$ is not always good. Based on it, we have VC-curve as follows:" } ]
